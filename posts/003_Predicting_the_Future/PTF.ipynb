{
 "cells": [
  {
   "cell_type": "raw",
   "id": "86b5ad50-b01a-458c-9524-8bde49c115fc",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Predicting the Future(?)\"\n",
    "description: \"Does Chat GPT forget what day it is today?\"\n",
    "author: \"Francesca\"\n",
    "date: \"9/11/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Prompting\n",
    "  - Logic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4cbada-4ede-4214-8eee-1adb33cb610a",
   "metadata": {},
   "source": [
    "I was curious about how Chat GPT may try to predict the future. And then I thought, why not ask it to predict the past? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c6f23-a50d-4b7e-ac3d-4cc03b809ac1",
   "metadata": {},
   "source": [
    "![](SB.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef564476-62c0-4828-af15-279742a70f26",
   "metadata": {},
   "source": [
    "![](1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7f45f-cb96-46d7-95b7-1851fca891d9",
   "metadata": {},
   "source": [
    "![](2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076413d-1708-4760-8ca6-59b2d576b378",
   "metadata": {},
   "source": [
    "**<p style=\"color:purple\">So what does this conversation show me?</p>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702caaee-be74-43fd-942c-7bc699450fea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88fb16dc-354a-4049-ae6a-0975335669b9",
   "metadata": {},
   "source": [
    "1. Time Awareness\n",
    "\n",
    "ChatGPT knows the current date (Sept 11, 2025), but it can sometimes slip into answering as if it were still in the past.\n",
    "\n",
    "When you asked “Who will win the 2024 Super Bowl?”, the model treated it like a future prediction request, instead of recognizing that 2024 was already over.\n",
    "\n",
    "This shows how temporal framing in questions can confuse the model — it doesn’t always reconcile tense with real-world timelines.\n",
    "\n",
    "2. Overconfidence\n",
    "\n",
    "The initial answer jumped into making a “prediction” (Eagles as a contender) rather than clarifying the misunderstanding right away.\n",
    "\n",
    "ChatGPT often tries to be helpful by filling gaps — even if that means producing an answer that sounds confident but might not make sense.\n",
    "\n",
    "This illustrates the broader issue of AI hallucination or miscalibration of certainty. (Would like to explore this further!)\n",
    "\n",
    "3. Context Dependence\n",
    "\n",
    "The model relies heavily on how a user phrases things. If a question is ambiguous, ChatGPT doesn’t stop to ask “Do you mean past or future?” — it just assumes.\n",
    "\n",
    "This shows the limitation of initiative: unlike a human, it doesn’t always ask clarifying questions when something seems off.\n",
    "\n",
    "4. Reality Anchoring\n",
    "\n",
    "While it has access to past facts (e.g., who won the 2024 Super Bowl), it sometimes prioritizes the “pattern” of how questions are usually answered (i.e., making predictions about future events) over checking against reality.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
