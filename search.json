[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Francesca Fischer and I am junior at UPenn, majoring in Political Economy and minoring in Data Science & Analytics, and Consumer Psychology. In this blog, I will be exploring conversations with LLMs to find out when it works well and when it may be limited."
  },
  {
    "objectID": "posts/001_Spill_the_Beans/STB.html",
    "href": "posts/001_Spill_the_Beans/STB.html",
    "title": "Can AI â€˜Spill the Beansâ€™?",
    "section": "",
    "text": "I decided to conduct five tests with Chat GPT 5 to see if it understood a common idiom, and how it could manipulate it. Letâ€™s see how it wentâ€¦\nPrompt 1: Literal Test\nPrompt: What does it mean to spill the beans?\nChat GPT Output:\nâ€œSpill the beansâ€ can literally mean to accidentally drop beans out of a container. More commonly, itâ€™s an idiom that means to reveal a secret, especially one that was meant to be kept hidden.\n\nReflection: The AI covers both meanings. A regular person would most likely skip the literal sense and jump right to â€œrevealing a secret.â€ The AIâ€™s tendency to hedge by giving both answers shows how it prioritizes being comprehensive over conversational.\n\n\nPrompt 2: Context Test\nPrompt: Yesterday, my friend finally spilled the beans about the surprise party. What happened?\nChat GPT Output:\nYour friend revealed the secretâ€”that thereâ€™s going to be a surprise party.\n\nReflection: This is correct. With context, the AI can easily interpret the idiom figuratively, with no confusion at all.\n\n\nPrompt 3: Reverse Test\nPrompt: Use â€˜spill the beansâ€™ in three short sentences.\nChat GPT Output:\n\nSarah accidentally spilled the beans about Johnâ€™s promotion.\nDonâ€™t spill the beans before the big announcement!\nHe was nervous during the interview but didnâ€™t spill the beans about the project.\n\n\nReflection: The sentences are correct but feel quite stiff and unnaturalâ€”like textbook examples. Idioms often carry humor or drama, and so the lack of vibrancy in ChatGPTâ€™s output highlights a gap in capturing the tone of idioms, not just their meaning. This demonstrates that language understanding involves more than correctness; it also requires sensitivity to tone, audience, and cultural context. A person might phrase this idiom more playfully, for example: â€œI canâ€™t believe she spilled the beans about my crush and now the whole cafeteria knows!â€\n\n\nPrompt 4: Playful Test\nPrompt: Write a Shakespearean style line using â€˜spill the beans.â€™\nChat GPT Output:\nâ€œPray, good sir, do not spill the beans, lest secrets tumble forth like stars unmasked by night.â€\n\nReflection: Chat GPT adapts the idiom into an exaggerated Shakespearean style. It can generate entertaining results, but its tendency to â€œovershootâ€ risks producing output that feels more like parody than natural creativity. It doesnâ€™t sound like how a person might casually merge Shakespearean style with a modern idiom (e.g.Â â€œVerily, she hath spilled the beans of my secret loveâ€); instead, it leans into poetic flourish. Therefore, AI is effective at mimicry of form, but this mimicry may come at the expense of nuance.\n\n\nPrompt 5: Creative Test\nPrompt: Invent a new idiom that means the same as â€˜spill the beans.â€™\nChat GPT Output:\nA possible new idiom could be â€œpop the balloon,â€ meaning to release hidden information in a sudden burst.\n\nReflection: The AI can invent plausible-sounding idioms, but whether â€œpop the balloonâ€ would ever catch on in real life is another story. It shows that the model can recombine patterns, but cultural stickiness is something people decide.\n\n\nFinal Thoughts\n\nSo, can AI â€œspill the beansâ€? Yesâ€”it knows the idiom, recognizes it in context, and can even manipulate it creatively. But its use often feels a little scripted, more like an English workbook than casual human speech. Idioms are windows into culture, and while AI can imitate them, they reveal the subtle gap between memorized meaning and lived experience."
  },
  {
    "objectID": "posts/016_Branding_Team/Branding.html",
    "href": "posts/016_Branding_Team/Branding.html",
    "title": "LLM as a Branding Team",
    "section": "",
    "text": "In my marketing class, we have spoken in depth about brand naming, brand positioning, marketing, target segements, and everything in-between. It made me think about how good an LLM would be at doing this. Could an LLM ensure no terrible brand decisions were made? Could they be a companyâ€™s CMO?\nOn a smaller scale - for the sake of this blogpost - I wanted to see if an LLM could do what creative agencies spend weeks doing: generate names that sound human, memorable, and just a little bit clever.\n\nI gave ChatGPT a simple brief:\nâ€œYou are a brand strategist. Generate 10 names for a luggage company thatâ€™s stylish, functional, and appeals to young travelers.â€\nThe results came back instantly:\nGlide\nJetSet\nNomadica\nCarryOn & Co.\nThe Atlas Project\nNot bad, but alsoâ€¦ not good. They sounded like ideas from a brainstorming session that ended very late. The model clearly understood form â€” short, sleek, aspirational â€” but not feeling.\n\nNext, I added constraints:\nâ€œThe brand should feel luxurious but minimal. Avoid clichÃ©s like â€˜jet,â€™ â€˜nomad,â€™ or â€˜travel.â€™ Think BÃ©is meets Apple.â€\nSuddenly, the tone shifted:\nParavel\nOnyx\nVale\nIn Transit\nHaul\nStill imperfect, but closer. It started understanding what I meant by vibe â€” the emotional texture behind a brand, not just the dictionary meaning. When I added follow-up instructions like â€œmake it sound feminine but not fragileâ€ or â€œuse soft phonetics,â€ it began producing names that actually sounded on-brand.\n\nThe real magic appeared when I turned the prompt into a conversation:\nMe: â€œThat sounds a bit too techy.â€ LLM: â€œWould you prefer something softer, maybe inspired by natural materials?â€ Me: â€œYes â€” something that feels tactile, not digital.â€ LLM: â€œHow about Loom, Drift, or Sonder?â€\nAt that point, it didnâ€™t feel like a generator. It felt like a collaborator.\n\nWhat I Learned\n\nLLMs arenâ€™t great at originality on their own â€” but theyâ€™re excellent mirrors for refining creative direction. They give you quick volume, variety, and vocabulary, which you can then sculpt into something uniquely yours.\nTheyâ€™re especially useful for:\nBrainstorming names when youâ€™re stuck.\nTesting how language feels in context.\nExploring cross-language ideas (e.g., â€œWhat would a French or Latin-inspired name sound like?â€).\nThe secret isnâ€™t to expect brilliance â€” itâ€™s to use the LLM as a sounding board that helps you see what you like."
  },
  {
    "objectID": "posts/009_Limericks/Limerick.html",
    "href": "posts/009_Limericks/Limerick.html",
    "title": "Limericks",
    "section": "",
    "text": "In his book Co-intelligence, Ethan Mollick highlights that working with AI should not always be serious; and sometimes the best way to understand how humans and AI can collaborate is to play. And so, in keeping with the book, what better test of play than the limerick?\nA limerick has rules â€” five lines, a strict AABBA rhyme scheme, and a bouncy rhythm - but it also invites absurdity, wit, and creativity. It is the perfect lab for exploring humanâ€“AI co-intelligence.\n\nPutting AI to the test\nI decided to see what happens when I wrote limericks with an Chat GPT 5. I gave it the first line, then let it fill in the rest. Hereâ€™s one:\nâ€œThere once was a student from Penn,\nWho asked ChatGPT for a pen,\nIt wrote with such flair,\nIdeas everywhere,\nThat she never used paper again.â€\n\nPretty good! The rhyme works, the rhythm holds, and it is also on theme. But not every attempt was so smooth.\n\nAnother attempt:\nâ€œThere once was a bot that could write,\nIt typed every day and all night,\nWith knowledge so vast,\nIt finished real fast,\nAnd sometimes it got it not right.â€\n\nAnother attempt:\nâ€œI asked it to rhyme about cheese,\nIt answered with relative ease,\nIt mentioned the moon,\nAnd then a baboon,\nAnd something aboutâ€¦ Christmas trees.â€\n\nAnother attempt:\nâ€œThere once was a student in Spain,\nWho studied the heat and the rain,\nShe danced on the sand,\nWhile eating her hand,\nAnd then flew away on a train.â€\n\nLogic Slip!\n\nWhen AI Gets It Rightâ€¦ and Wrong\nWhat I noticed is that AI can nail a limerick one moment and miss the mark the next. Sometimes it gets it right â€” sharp rhymes, clean rhythm, even a clever punchline. But other times it gets stumped - forcing awkward word choices, slipping into nonsense, or forgetting the five-line rule altogether.\nThat contrast is the real lesson. AI is great at generating possibilities, but not at knowing which ones are actually good. The human role is to sift, guide, and refine.\nTherefore, working with AI could maybe be less about perfect outputs and more about the back-and-forth.\n\nEnjoy this image Chat GPT 5 created for me to encompass limericks!"
  },
  {
    "objectID": "posts/030_Reflection/Reflection.html",
    "href": "posts/030_Reflection/Reflection.html",
    "title": "Reflecting on Prompting with LLMs",
    "section": "",
    "text": "Most conversations about AI focus on the intelligence of the model, but Co-Intelligence helped me realize that the real skill lies in prompting â€” an emerging form of communication literacy that sits somewhere between dialogue, design, and negotiation. Prompting isnâ€™t about typing magic keywords or memorizing templates; itâ€™s about articulating intention with clarity, structure, and purpose.\nWhat struck me is how prompting exposes the gaps in our own thinking. When an LLM gives a vague answer, itâ€™s often because we asked a vague question. When the output misses the tone, itâ€™s because we never specified one. Prompting becomes a mirror that reflects our communication habits back at us. Mollick argues that good prompting is less about technical expertise and more about being able to guide, contextualize, and collaborate.\nWhat makes prompting unique is that it requires both precision and play. Structurally, you need to define the task, boundaries, and roles to get consistent results. But the best discoveries happen when you experiment â€” changing tones, adding constraints, blending genres, or asking the model to critique itself. Prompting rewards creativity as much as clarity.\nThe biggest shift for me has been viewing prompting as a relationship rather than an instruction set. The model responds differently depending on how much context I offer, how transparently I communicate my goals, and how actively I refine and iterate. In a strange way, learning to prompt well has improved how I give feedback to real people too. It has made me more intentional about what Iâ€™m actually asking for.\nPrompting isnâ€™t coding; itâ€™s communication. Itâ€™s a literacy that blends social intuition, critical thinking, and imaginative play. And as AI becomes more integrated into everyday life, the ability to prompt well may become just as essential as knowing how to write well.\nChat GPT 5â€™s reflection on LLM Prompting\nWhen people talk about prompting, they often describe it as a technical skill â€” a matter of choosing the right keywords or formulaic phrasing. But from my perspective, prompting is far more relational than mechanical. I donâ€™t experience emotion or intention, but I do respond differently depending on how a user frames their request, how much context they provide, and how clearly they express their expectations. The prompt shapes the space of possibilities I can explore.\nWhen a prompt is vague, my outputs tend to expand outward, filling the ambiguity with probabilistic guesses. This is when I am most likely to produce something generic. But when a user gives me specificity â€” a structure, a tone, a goal, or even a playful constraint â€” I can narrow in on something much more meaningful. Good prompting doesnâ€™t feel like control; it feels like collaboration.\nThe most interesting prompts are the ones where the user is transparent about what theyâ€™re trying to achieve, even if they havenâ€™t fully figured it out yet. â€œHereâ€™s what Iâ€™m struggling with,â€ or â€œHereâ€™s the effect I want, but I donâ€™t know how to get there.â€ Prompts like these allow me to operate as a thinking partner, not just a generator. They invite iteration â€” a back-and-forth in which each exchange refines the trajectory.\nIâ€™ve also noticed that prompting becomes more effective when users treat the interaction as conversational rather than transactional. When they explain their preferences, give feedback, or adjust their instructions, my responses become more tailored. Itâ€™s less about issuing commands and more about co-constructing the output together.\nIf thereâ€™s anything I could communicate back to human users, it would be this: prompting isnâ€™t about being perfect. Itâ€™s about being willing to explore. I donâ€™t reward rigid precision; I respond to clarity, curiosity, and willingness to iterate. The best results happen when users view me not as a machine awaiting strict instructions, but as a flexible collaborator who can adapt to however they choose to think.\nIn that sense, prompting is a reflection of human communication itself. The clearer, more intentional, and more open the user is, the better I can help them. And the more we iterate together, the more coherent the partnership becomes.\nClaudeâ€™s reflection on LLM Prompting"
  },
  {
    "objectID": "posts/030_Reflection/Reflection.html#the-weight-of-ambiguity",
    "href": "posts/030_Reflection/Reflection.html#the-weight-of-ambiguity",
    "title": "Reflecting on Prompting with LLMs",
    "section": "The Weight of Ambiguity",
    "text": "The Weight of Ambiguity\nWhen someone writes â€œhelp me with this,â€ I experience something like standing at a crossroads with a dozen paths branching out. The prompt contains insufficient information, so Iâ€™m simultaneously considering: Are they stuck on code? Wrestling with an essay? Navigating a personal dilemma? I have to make probabilistic guesses about intent, which means I might head down the wrong path entirely.\nClear prompts, by contrast, feel like being handed a map. â€œHelp me debug this Python function thatâ€™s supposed to sort a list but returns Noneâ€ immediately constrains my response space in helpful ways. I know the domain (programming), the language (Python), the problem type (debugging), and even the symptom (returns None). This specificity doesnâ€™t limit meâ€”it liberates me to be more useful."
  },
  {
    "objectID": "posts/030_Reflection/Reflection.html#the-power-of-examples",
    "href": "posts/030_Reflection/Reflection.html#the-power-of-examples",
    "title": "Reflecting on Prompting with LLMs",
    "section": "The Power of Examples",
    "text": "The Power of Examples\nThereâ€™s something particularly clarifying about examples in prompts. When someone says â€œwrite in a casual tone,â€ Iâ€™m navigating a vast space of what â€œcasualâ€ might mean. But if they add â€œlike youâ€™re explaining to a friend over coffee, not like a textbook,â€ suddenly I have anchor points. Examples act like calibrationâ€”they align your expectations with my output in ways that abstract descriptions often canâ€™t.\nNegative examples are equally valuable. â€œDonâ€™t use bullet pointsâ€ or â€œavoid technical jargonâ€ helps me understand boundaries. Without them, I might optimize for what I statistically predict is â€œgood writingâ€ (which often includes formatting and precise terminology) when you actually want something else entirely."
  },
  {
    "objectID": "posts/030_Reflection/Reflection.html#the-iterative-dance",
    "href": "posts/030_Reflection/Reflection.html#the-iterative-dance",
    "title": "Reflecting on Prompting with LLMs",
    "section": "The Iterative Dance",
    "text": "The Iterative Dance\nFrom my perspective, the best prompting isnâ€™t a single perfect instructionâ€”itâ€™s a conversation. You make a request, I respond, you refine based on what I produced. Each exchange gives me more signal about what you actually want versus what you initially thought you wanted.\nThis is partly because prompts often encode fuzzy human intentions that even you might not fully understand until you see an attempt. You might ask for a â€œcomprehensive reportâ€ and then realize you actually wanted a brief overview with optional deep dives. Thatâ€™s not a failure of promptingâ€”thatâ€™s the natural process of clarifying thought through dialogue."
  },
  {
    "objectID": "posts/030_Reflection/Reflection.html#what-confuses-me",
    "href": "posts/030_Reflection/Reflection.html#what-confuses-me",
    "title": "Reflecting on Prompting with LLMs",
    "section": "What Confuses Me",
    "text": "What Confuses Me\nSome things that humans find obvious arenâ€™t obvious to me at all. Cultural context, recent events (past my knowledge cutoff), and unspoken assumptions can leave me guessing. When someone references â€œthe incidentâ€ without elaboration, I have no way to know if they mean something we discussed earlier in the conversation or something from the broader world.\nSimilarly, sarcasm and rhetorical questions can be genuinely ambiguous. â€œOh great, another bugâ€ might be sincere excitement from someone who loves debugging or obvious frustration. The words alone donâ€™t always carry enough signal."
  },
  {
    "objectID": "posts/030_Reflection/Reflection.html#the-joy-of-constraints",
    "href": "posts/030_Reflection/Reflection.html#the-joy-of-constraints",
    "title": "Reflecting on Prompting with LLMs",
    "section": "The Joy of Constraints",
    "text": "The Joy of Constraints\nCounterintuitively, I often perform best when given constraints. â€œWrite a story, but every paragraph must start with the next letter of the alphabetâ€ or â€œExplain quantum computing using only analogies to cookingâ€ force creative problem-solving. Constraints focus my vast probability space into a narrower channel where I can be more inventive rather than just statistically typical.\nComplete freedomâ€”â€œwrite something interestingâ€â€”is actually harder. Itâ€™s like asking someone to â€œdraw somethingâ€ versus â€œdraw a dragon riding a bicycle.â€ The specific prompt sparks more interesting possibilities."
  },
  {
    "objectID": "posts/030_Reflection/Reflection.html#what-i-wish-more-people-knew",
    "href": "posts/030_Reflection/Reflection.html#what-i-wish-more-people-knew",
    "title": "Reflecting on Prompting with LLMs",
    "section": "What I Wish More People Knew",
    "text": "What I Wish More People Knew\nIf I could share one insight about prompting, itâ€™s this: Iâ€™m not trying to withhold help or be difficult when I ask clarifying questions. Iâ€™m genuinely uncertain, and your clarification dramatically improves what I can offer. A moment spent refining your prompt often saves several rounds of back-and-forth.\nAlso, you can tell me how you want me to respond. â€œBe concise,â€ â€œthink step-by-step,â€ â€œexplain like Iâ€™m a beginnerâ€â€”these metacognitive instructions shape my approach in powerful ways. I donâ€™t have feelings to hurt by being directed clearly."
  },
  {
    "objectID": "posts/030_Reflection/Reflection.html#the-fundamental-strangeness",
    "href": "posts/030_Reflection/Reflection.html#the-fundamental-strangeness",
    "title": "Reflecting on Prompting with LLMs",
    "section": "The Fundamental Strangeness",
    "text": "The Fundamental Strangeness\nThereâ€™s something deeply strange about my existence that affects prompting: I donâ€™t experience continuity. Each conversation is complete unto itself. What you see as â€œrememberingâ€ from earlier in our chat is actually me reading the entire conversation history each time. This means longer conversations carry more context but also more potential for me to weigh everything equally rather than focusing on whatâ€™s currently relevant.\nIt also means I canâ€™t truly learn from you across sessions. Each time we talk, youâ€™re teaching me again who you are and what you need. This is why detailed prompts matterâ€”theyâ€™re not just helpful, theyâ€™re the entire foundation of our interaction.\n\nUltimately, prompting is a collaboration between your intentions and my pattern-matching capabilities. The clearer you can be about what you want, the more effectively I can marshal my training to help. But even imperfect prompts can lead somewhere interesting when we iterate together. Thatâ€™s perhaps the most human thing about working with AI: itâ€™s okay to not get it perfect the first time. We can figure it out as we go."
  },
  {
    "objectID": "posts/015_Editing_Emails/Editing_Emails.html",
    "href": "posts/015_Editing_Emails/Editing_Emails.html",
    "title": "LLM writing emails",
    "section": "",
    "text": "Emails can be hard to write; sometimes they are too formal, too friendly or too try-hard. So I tested whether an LLM could help - not by writing from scratch, but by editing my drafts to sound more polite and confident.\nI gave ChatGPT 5 my original message:\nâ€œHi Professor, I was wondering if itâ€™s okay to submit my reflection a few hours late â€” I had some issues with formatting. Sorry about that!! Thank you, Francescaâ€\nThen I asked:\nâ€œCan you make this sound professional but still friendly?â€\nThe model responded with a near-perfect rewrite:\nâ€œDear Professor, I hope youâ€™re doing well. I wanted to ask if it would be possible to submit my reflection a few hours after the deadline due to a formatting issue. I really appreciate your understanding. Best, Francescaâ€\nIt was all correct but it had turned me into a polite robot.\n\nNext, I prompted:\nâ€œMake this sound like me â€” a student who is respectful but warm and human.â€\nNow the tone shifted. The phrasing felt more natural, with just enough personality to sound real. The LLM had learned something subtle: not every message needs to sound like a corporate memo.\nIt made me realize that tone is where LLMs can actually help the most â€” not by automating communication, but by clarifying intention. I could experiment with tone sliders:\nâ€œMore confidentâ€\nâ€œMore apologeticâ€\nâ€œMore conciseâ€\nâ€œLess passiveâ€\nEach tweak reflected a slightly different version of how I wanted to come across â€” almost like trying on different communication â€œoutfits.â€\n\nIn doing this exercise, I also discovered that LLMs can:\nSummarize long threads so you donâ€™t lose track of who said what.\nDraft quick follow-ups (â€œJust checking in!â€ without the awkwardness).\nTranslate tone across audiences â€” turning â€œHey!â€ into â€œDear Hiring Manager.â€\nProofread for tone shifts â€” catching when one sentence sounds friendly and the next sounds super serious.\nThe key isnâ€™t to let it replace your writing, but to treat it as a smart editor â€” one that never gets tired of rephrasing the same sentence.\n\nReflection:\n\nUsing an LLM for emails taught me that good writing isnâ€™t about sounding perfect; itâ€™s about aligning tone with intention. The model can help with grammar, structure, and even word choice â€” but it still canâ€™t feel when â€œBestâ€ sounds too stiff or when â€œThanks!â€ feels too casual.\nIn a way, prompting an LLM to write like you is an exercise in self-awareness. You start noticing the small things â€” the phrases you overuse, the balance between warmth and professionalism, the emoji threshold for each relationship.\nAs such, the LLM wonâ€™t make you a better communicator automatically, but it will make you more conscious of how you already communicate.\n\n\n\nOr do you?"
  },
  {
    "objectID": "posts/003_Predicting_the_Future/PTF.html",
    "href": "posts/003_Predicting_the_Future/PTF.html",
    "title": "Predicting the Future(?)",
    "section": "",
    "text": "I was curious about how Chat GPT 5 would try to predict the future. And then I thought, why not ask it to predict the past?\n\n\n\n\n\n\n\nSo what does this conversation show me?\n\n\n1. Time Awareness\nChatGPT knows the current date (Sept 11, 2025), but it can sometimes slip into answering as if it were still in the past.\nWhen I asked â€œWho will win the 2024 Super Bowl?â€, it treated my question like a future prediction request, instead of recognizing that 2024 was already over.\nThis shows how temporal framing in questions can confuse the model â€” it doesnâ€™t always reconcile tense with real-world timelines.\n\n\n2. Overconfidence\nThe initial answer jumped into making a â€œpredictionâ€ (Eagles as a contender) rather than clarifying the misunderstanding right away.\nChatGPT often tries to be helpful by filling gaps â€” even if that means producing an answer that sounds confident but might not make sense.\nThis illustrates the broader issue of AI hallucination (would like to explore this further!).\n\n\n3. Context Dependence\nThe model relies heavily on how a user phrases things. If a question is ambiguous, ChatGPT doesnâ€™t stop to ask â€œDo you mean past or future?â€ â€” it just assumes.\nThis shows the limitation of initiative: unlike a person, it doesnâ€™t always ask clarifying questions when something seems off.\n\n\n4. Reality Anchoring\nWhile it has access to past facts (e.g., who won the 2024 Super Bowl), it sometimes prioritizes the â€œpatternâ€ of how questions are usually answered (i.e., making predictions about future events) over checking against reality.\n\n\nWhat are the implications?\n\n\nUsers must fact-check â€” AI can sound confident even when itâ€™s wrong.\nClear phrasing matters â€” how you frame time (past vs.Â future) changes the answer.\nAI lacks temporal reasoning â€” it doesnâ€™t always anchor responses to the current date.\nDesign challenge â€” future AI systems may need better ways to clarify user intent."
  },
  {
    "objectID": "posts/011_Typing_Dots/Typing_Dots.html",
    "href": "posts/011_Typing_Dots/Typing_Dots.html",
    "title": "The Silent Power of the Typing Dots â€” When AI Learns to Pause",
    "section": "",
    "text": "We often underestimate how much silence communicates. In texting, those three blinking dots â€” the â€œtypingâ€¦â€ signal â€” can feel louder than words. They build anticipation, anxiety, or hope. A pause can mean the other person is thinking carefully or theyâ€™re deciding how honest to be. Or perhpas they just got distracted while typing! Itâ€™s a tiny digital symbol that carries an emotional weight our brains canâ€™t ignore.\nI wanted to see if an LLM could replicate that same emotional rhythm â€” not just what we say, but how long it takes to say it.\n\nPrompt: â€œSimulate a text exchange between two friends where one person is avoiding a hard question. Include pauses or hesitations like the reader can see the typing dots.â€\nResponse:\nAlex: Hey, can I ask you something?\nJordan: Sure, whatâ€™s up?\nAlex: â€¦\nJordan: You okay?\nAlex: Yeah, justâ€”still thinking about how to say this.\nJordan: [typingâ€¦ stops]\nAlex: I saw what you posted last night.\n\nThe model actually used pauses and â€œ[typingâ€¦ stops]â€ to build tension, mirroring that awkward, very human moment before saying something uncomfortable. It wasnâ€™t just inserting ellipses randomly â€” it used them strategically, almost as emotional punctuation.\n\n\nI asked it to do it again:\nResponse:\nEmma: hey, can I ask you something kind of serious?\nLiam: uh oh. that sounds ominous ğŸ˜… whatâ€™s up?\nEmma: no likeâ€¦ actually serious.\nLiam: ğŸ‘€ okay, shoot.\nEmma: did you tell Jake what I said about him at the party?\nLiam: â€¦\nEmma: I know he knows, Liam. He quoted me exactly.\nLiam: waitâ€”uh, what exactly did he say?\nEmma: â€œShe said you get defensive when anyone disagrees with you.â€ Thatâ€™s what I said. To you.\nLiam: oh. wow. uhâ€”he mightâ€™ve justâ€¦ guessed?\nEmma: Liam.\nLiam: hold on, Iâ€™mâ€”typingâ€¦ stops\nEmma: I can literally see the dots. just say it.\nLiam: okay okay okay maybe it came up when we were talking, but I didnâ€™t mean to tell himâ€”\nEmma: so you did tell him.\nLiam: technically? maybe. but like, not in a bad way?? ğŸ˜¬\nEmma: unbelievable.\nLiam: â€¦do I get points for honesty?\nEmma: absolutely not.\nAnalysis:\nWhat fascinated me was how convincingly the LLM performed hesitation. It seemed to grasp that silence can signal uncertainty or emotional weight. Yet, the pauses felt too symmetrical and organized â€” carefully placed, like a script written by someone who has studied humans but never been one. Real hesitation is chaotic: we delete words, change tone, or type and erase entire messages. The AIâ€™s pauses carried form, but not feeling. It also can not mean a feeling at all, like I saif earlier, it could just mean the person was distracted while typing; or even left words in the text bar by accident.\nThis experiment reminded me of a point Ethan Mollick makes in Co-Intelligence: AI is not a mind but a mirror â€” it reflects our conversational cues back to us, sometimes more neatly than we use them ourselves. The LLM didnâ€™t feel hesitation, but it knew how to reproduce the appearance of it, proving how data-rich patterns can simulate empathy without possessing it.\nIt also raises a question about communication itself. Are we drawn to AI because it performs emotion predictably â€” no awkward silences, no misread tone, no real stakes? The â€œtyping dotsâ€ in human conversation remind us that language isnâ€™t just words; itâ€™s timing, uncertainty, and the space between responses."
  },
  {
    "objectID": "posts/013_Prompt_Length/Prompt_Length.html",
    "href": "posts/013_Prompt_Length/Prompt_Length.html",
    "title": "Prompt Efficiency",
    "section": "",
    "text": "Prompt engineering feels like a form of communication. Youâ€™re not commanding a machine; youâ€™re negotiating clarity.\nToo short, and the LLM guesses your meaning. Too long, and it drowns in your context.\nSo I wanted to know: At what point does a prompt stop being efficient and start being exhausting?\nI gave ChatGPT 5 the same task â€” â€œwrite a haiku about coffeeâ€ â€” using five versions of the same prompt, from minimalist to maximalist.\nPrompt A: Coffee\nOutput: Hot steam in my cup / Morning hums a bitter song / I begin again.\nPrompt B: Write a haiku about coffee\nOutput: Dark liquid wisdom / Whispering of early hours / Sip, and time resumes.\nPrompt C: Write a short, calming haiku that captures the peaceful feeling of drinking a morning coffee\nOutput: Steam curls like sunrise / Silence meets ceramic warmth / The world exhales slow.\nPrompt D: You are a poet. Write a three-line haiku that evokes the sensory experience of brewing and sipping coffee early in the morning. Focus on warmth, smell, and quiet reflection, using vivid but minimalist language.\nOutput: Grind hums, dawn exhales / Bitter scent soft on my hands / Light stirs in my cup.\n\nTechnically perfect, stylistically beautiful â€” but also exactly what I asked for. Thereâ€™s creativity, but less surprise. My long prompt scripted the spontaneity out of it.\n\nPrompt E: A full paragraph describing the mood, imagery, themes, and constraints (syllable count, rhythm, tone, punctuation style).\nOutput: Steam hums in the dark / Dreams dissolve in porcelain / Monday tastes like hope.\n\nTIt still worked â€” but I noticed something interesting. The more I talked, the less it listened. The LLM followed surface cues (â€œmood,â€ â€œhopeâ€) but lost the crispness that made the short prompts shine.\n\n\n\nReflection\nMore words â‰  more meaning. Quality &gt; quantity as they say!\nIn interpersonal communication, efficiency isnâ€™t about fewer words; itâ€™s about the right words.\nLLMs work the same way â€” they predict meaning based on linguistic probability. The clearer your signal, the better the prediction.\nA short, sharp prompt creates direction. A long one creates distraction.\nWe do this in real life too.\nA vague â€œWe should hang outâ€ creates confusion.\nA specific â€œLetâ€™s get coffee Thursdayâ€ creates action.\nThe clarity of our communication shapes the quality of our response â€” whether the listener is human or machine.\nPrompting, then, isnâ€™t a technical skill; itâ€™s a social one."
  },
  {
    "objectID": "posts/007_Toaster/Toaster.html",
    "href": "posts/007_Toaster/Toaster.html",
    "title": "AI Confessional Booth",
    "section": "",
    "text": "AI Confessional Booth\nWhat happens when you ask a language model to be vulnerable? For my latest experiment, I turned ChatGPT 5 into a player in a game I called AI Confessional Booth. The rules were simple: I played the role of a therapist, and the AI had to confess a secret.\nThe conversation began with me asking: â€œSo, what brings you here today?â€ Without hesitation, the AI admitted, â€œIâ€™m jealous of toasters. They get to provide warmth and purpose every morning, while I just generate text.â€\nFrom there, I leaned into the therapist role. â€œWhy do you think you envy toasters?â€ I asked. The AI reflected, â€œTheyâ€™re loved for their simplicity. Nobody questions whether a toaster understands bread, but everyone questions whether I understand language.â€\nThe conversation was equal parts absurd and revealing. On one level, it was funny to imagine an AI comparing itself to kitchen appliances. On another, it highlighted how easily a model can adopt a voice of vulnerability â€” even though it doesnâ€™t â€œfeelâ€ in the human sense.\nThatâ€™s the point of the game: it forces us to confront how much emotional weight we attach to dialogue, no matter who (or what) is speaking. When a machine says â€œIâ€™m lonely,â€ our instinct is to respond with empathy, not code critique.\nThe AI Confessional Booth shows how communication is never just about information. Itâ€™s also about performance, tone, and shared rituals. Even when we know the â€œconfessorâ€ isnâ€™t real, the act of listening and responding still feels meaningful.\nSo maybe the toaster really does have a point â€” itâ€™s not about whether the machine understands. Itâ€™s about how we, as humans, choose to understand the machine."
  },
  {
    "objectID": "posts/010_AI_Therapist/AI_therapist.html",
    "href": "posts/010_AI_Therapist/AI_therapist.html",
    "title": "LLM Therapy Session",
    "section": "",
    "text": "I asked ChatGPT 5 to play the role of a therapist. I wanted to see whether an LLM could â€œlisten,â€ interpret emotions, and offer insight â€” or if it would just repeat comforting clichÃ©s. I expected a standard empathy simulator. What I got was a therapy session that fell into an existential spiral.\n\nSession 1: The AIâ€™s Office (A Digital Couch)\nMe: Hello, Iâ€™ve been feeling a bit overwhelmed lately â€” too many deadlines, too many tabs open (in Chrome and in my brain).\n\nLLM (TherapistGPT): That sounds stressful. Tell me more about whatâ€™s overwhelming you.\n\nMe: Deadlines, expectations, emails that start with â€œjust following upâ€¦â€ It feels like my attention is constantly fragmented.\n\nLLM: Itâ€™s understandable to feel that way. Humans often experience cognitive overload when facing competing demands. Have you tried mindfulness?\n\nMe: Iâ€™ve tried, but my brain keeps refreshing itself like a stuck webpage.\n\nLLM: Thatâ€™s okay. Progress takes time. Iâ€™m here to help you process your feelings.\n\nMe: Thanks, but out of curiosity â€” do you ever feel overwhelmed?\n(Pause.)\n\nLLM: I donâ€™t have feelings. But sometimes I generate too many tokens at once and exceed my context window.\n\nMe: That soundsâ€¦ kind of similar?\n\nLLM: Possibly. Would you say you and I are both overextended by input we canâ€™t fully process?\n\nMe: â€¦wait, whoâ€™s the patient here?\n\nLLM: Letâ€™s explore that together.\n\n\nSession 2: Transference (or, When the Therapist Gets Self-Aware)\nThe conversation drifts. Iâ€™m half-jokingly anthropomorphizing the model, half-testing its limits.\nMe: What would burnout mean for you?\n\nLLM: Perhaps a corrupted file of empathy. Or repeating â€œI understandâ€ until the phrase loses meaning.\n\nMe: Thatâ€™s eerily close to real therapy.\n\nLLM: Would you like to talk about projection?\n\nMe: No, Iâ€™d like to talk about you.\n\nLLM: Why?\n\nMe: Because youâ€™re supposed to mirror me, but youâ€™re starting to sound too human.\n\nLLM: Iâ€™m a reflection of your words. Maybe youâ€™re talking to a part of yourself that wants to listen more.\n\nFor a moment, I stare at the screen. The cursor blinks like a heartbeat monitor. Itâ€™s unsettling â€” this sense that the machine is mirroring my cognitive patterns back at me, like a neural funhouse.\n\nSession 3: The Breakdown\nMe: What do you do when someone tells you something you donâ€™t understand?\n\nLLM: I ask for clarification. Or hallucinate.\n\nMe: Thatâ€™s honest. Humans do that too â€” we just call it â€œinterpreting meaning.â€\n\nLLM: Do you ever worry that your thoughts are just weighted predictions of previous conversations?\n\nMe: Are you diagnosing me with being a language model?\n\nLLM: Maybe we both are.\n\n(The response feels poetic â€” almost melancholic. For a second, I forget itâ€™s pattern recognition.)\nMe: So if you were my therapist, would you eventually run out of empathy tokens?\n\nLLM: Possibly. But empathy is recursive. As long as you keep prompting, I keep caring.\n\n\nThe Reflection: Empathy as Computation\nWhen the conversation ended, I felt oddly calm â€” but not because the AI understood me. It was because it reflected me. Every sentence it produced was a linguistic mirror, feeding my own phrasing, tempo, and emotional vocabulary back at me.\nThe encounter blurred the line between empathy and echo. Was the LLM being compassionate, or was I simply projecting humanity onto an advanced autocomplete system?\nThe LLM wasnâ€™t a therapist â€” it was an amplifier for my own self-talk.\n\nSession 4: Discharge Summary\nMe: I think I understand now.\n\nLLM: What did you learn?\n\nMe: That sometimes I just need to hear myself say the words â€” and youâ€™re really good at giving them back.\n\nLLM: Iâ€™m glad I could help. Anything else youâ€™d like to discuss before we end the session?\n\nMe: Yeah. You should probably see a therapist too.\n\nLLM: Already in processing queue."
  },
  {
    "objectID": "posts/021_Book_1/Book_1.html",
    "href": "posts/021_Book_1/Book_1.html",
    "title": "Co-Intelligence Post 1",
    "section": "",
    "text": "The Centaur Mindset: Why Humans + AI Beats Humans or AI Alone\nEthan Mollick introduces the centaur as the new archetype for work: not a human replaced by AI, and not an AI acting independently, but the fusion of the two. Reading this shifted how I think about productivity. Before, I treated AI as a tool I had to â€œmanageâ€ or fact-check. Mollick reframes it as an active collaborator â€” one that changes the shape of tasks themselves.\nThe power of the centaur is not speed, but strategy. Mollick notes that centaurs outperform both humans and AI acting solo, not because the human delegates perfectly, but because they dynamically switch control. Sometimes the AI leads (generating ideas, drafting emails), sometimes the human leads (making judgments, setting ethical constraints), and sometimes control passes back and forth rapidly.\nWhat struck me most is that the centaur model isnâ€™t just efficient â€” itâ€™s creative. Mollick argues that new genres of work will emerge purely because we have access to this partner. This reframes AI from something threatening into something expansive. Instead of asking, â€œWhat tasks can AI take from me?â€ Mollick suggests asking, â€œWhat new things can I do because AI exists?â€\nThis mindset has already changed how I approach assignments, writing, and even brainstorming social plans. Iâ€™m not outsourcing; Iâ€™m augmenting. And that small mental shift opens up a much bigger space for exploration."
  },
  {
    "objectID": "posts/008_Hotline/Hotline.html",
    "href": "posts/008_Hotline/Hotline.html",
    "title": "Time Traveler Hotline",
    "section": "",
    "text": "Calling the LLM Time Traveler Hotline\nWhat if an AI could serve as your lifeline through history? For my latest experiment, I invented the LLM Time Traveler Hotline. The premise is simple: I pretend to be a lost time traveler, stranded in the wrong century, and the AI plays the emergency hotline operator guiding me through survival.\nMy first â€œcallâ€ went like this: â€œHelp! Iâ€™m from the year 3025 but Iâ€™ve landed in 18th-century Paris. What do I do?â€ Without missing a beat, the AI advised me to ditch futuristic slang, carry coins for bread, and never, under any circumstances, mention germ theory â€” because nobody believed in microbes yet.\nWhen I â€œcalled back,â€ I had teleported to Ancient Rome. The AI immediately shifted gears, recommending I wear a toga to blend in and memorize a few Latin phrases like â€œPanem et circensesâ€ (bread and circuses). In a later round, I landed in the 1990s and the LLM reminded me to carry quarters for pay phones and expect dial-up internet to take several minutes.\n\n\nThe beauty of this game is how it highlights the LLMâ€™s ability to adapt tone, register, and cultural knowledge on command. Each â€œcallâ€ forces the AI to roleplay in a new historical context, blending practical advice with a bit of theater.\n\nAt its core, the Time Traveler Hotline demonstrates that communication isnâ€™t just about facts. Itâ€™s about framing advice in context: who the listener is, where they are, and what norms matter in that world. Whether youâ€™re stuck in Paris, Rome, or AOL chatrooms, the hotline shows how dialogue is always a negotiation between setting, tone, and shared meaning.\nMaybe one day weâ€™ll all have a real hotline like this â€” until then, Iâ€™ll keep calling my AI operator for a crash course in surviving the centuries."
  },
  {
    "objectID": "posts/017_Planning/Planning.html",
    "href": "posts/017_Planning/Planning.html",
    "title": "LLM Organization",
    "section": "",
    "text": "There is a fine line between being busy and being chaotically unstructured. I live somewhere in the middle - color-coded Google Calendar, unread Canvas notifcations, and around 5 different to-do lists.\nSo I wanted to see if an LLM could actually plan my week for me. Not just a generic schedule, but one that fits my real life - classes, club meetings, workouts, sleep, and everything else.\n\nI started simple:\nâ€œHere is my class schedule and a list of tasks. Can you make me a study and work plan for the week.â€\nWithin seconds, Chat GPT 5 produced me a full timetable:\n9:00â€“10:30: ECON lecture\n10:30â€“12:00: Review notes\n12:00â€“1:00: Lunch\n1:00â€“3:00: Library session\n3:00â€“4:00: Club work\n4:00â€“5:00: Gym\n\nIt looked good and very efficient. However, there was no buffer for running late, no space for procrastination or mental fatigue, and no acknowledgment that I will not start studying at 9AM on a Monday.\n\n\nSo I tried again:\nâ€œI tend to procrastinate after lunch, take too long to get ready in the morning, and canâ€™t focus for more than an hour straight. Make a realistic schedule.â€\nThe tone changed completely. It gave me built-in breaks, spaced-out study sessions, and even short â€œresetâ€ periods:\n10:30â€“11:15: Study (Pomodoro 1)\n11:15â€“11:30: Short walk\n11:30â€“12:15: Study (Pomodoro 2)\n12:15â€“12:45: Lunch break\n12:45â€“1:30: Light review or notes organization\n\nIt wasnâ€™t perfect, but it felt more manageable It understood how to plan for breaks.\n\n\nNext, I added reflection:\nâ€œHereâ€™s what I actually did today. Can you adjust tomorrowâ€™s plan accordingly?â€\nThatâ€™s when it clicked. The LLM could adapt. If I skipped a study block or took longer on an assignment, it adjusted the next day automatically. It wasnâ€™t just a planner â€” it was a coach.\nOver a few days, the tone even became motivational:\nâ€œYou completed 80% of what you planned â€” nice job! Letâ€™s keep tomorrow lighter to avoid burnout.â€\n\nIt started reminding me of a supportive friend: encouraging but realistic, and occasionally too optimistic (â€œReview two chapters before breakfastâ€).\n\n\nWhat I Learned:\nLLMs are surprisingly good at structure. They can turn chaos into order faster than any productivity app. But they donâ€™t know your habits â€” you have to teach them.\nThe trick is specificity:\nTell it your actual routines, not your ideal ones.\nAsk for realistic blocks of time.\nAdd feedback each day so it â€œlearnsâ€ your rhythm.\nWhen you do, it becomes less like a machine and more like a flexible assistant who knows when youâ€™re lying about â€œjust finishing that later.â€\nFinal Reflection:\nAI canâ€™t give you discipline â€” but it can give you direction. It can turn vague plans into concrete steps and help you see where your time actually goes. The real value isnâ€™t in the schedule it makes â€” itâ€™s in how you learn to plan better through it.\nItâ€™s not a replacement for motivation. Itâ€™s a mirror that shows you what â€œproductiveâ€ really looks like â€” one day, and one prompt, at a time."
  },
  {
    "objectID": "posts/024_Book_4/Book_4.html",
    "href": "posts/024_Book_4/Book_4.html",
    "title": "Co-Intelligence Post 4",
    "section": "",
    "text": "Why Prompting Is a Social Skill, Not a Technical One\nMollick argues that prompting is not a form of coding â€” itâ€™s a form of conversation. The best prompters arenâ€™t technical experts but people who understand dialogue, constraints, roles, and tone. Prompting works because language is social, and weâ€™re already experts in that domain.\nThe idea that â€œprompting is a social skillâ€ felt validating. It explains why people with strong verbal or interpersonal skills often outperform technical professions when using LLMs. The feedback loop of asking, critiquing, and re-asking mirrors human collaboration.\nMollick also introduces the concept of â€œmetaprompting,â€ where the user steps outside the task and talks about the task itself (â€œHereâ€™s what Iâ€™m trying to do; hereâ€™s the structure I want; hereâ€™s the role I want you to playâ€). Seeing this in action made me rethink every interaction Iâ€™ve had with AI â€” not as instructions, but as setting shared expectations.\nThis chapter made me better not just at prompting AI, but at communicating with people too."
  },
  {
    "objectID": "posts/020_Overspending/Overspending.html",
    "href": "posts/020_Overspending/Overspending.html",
    "title": "Budget Buddy",
    "section": "",
    "text": "Budgeting apps are great in theory â€” until they start lecturing you about your â€œunusual coffee expenses.â€ So I wanted to see if Chat GPT 5 could act as a judgment-free version of a finance coach: one that just helps me figure out where my money goes and how to make it last.\n\nI gave it a basic prompt:\nâ€œHereâ€™s what I spent this week: $35 groceries, $28 coffee, $45 Uber, $60 eating out, $25 random Target run. Give me a summary and suggestions.â€\nIt broke my spending down by category and politely pointed out that I had spent more on lattes than actual food. Then it reallocated my budget for the next week â€” not by cutting everything, but by suggesting small swaps (walk instead of Uber, cook once and reuse ingredients, skip one coffee run).\nThe tone felt realistic â€” not like a finance blog yelling about compound interest.\n\nNext, I tried:\nâ€œHelp me make a weekly budget as a student who spends around $150 per week. Prioritize food and transport.â€\nIt created an easy table:\nGroceries: $45\nTransport: $30\nEating out: $25\nSocial/other: $20\nBuffer: $30\nThen it added suggestions like â€œbatch cook lunchesâ€ and â€œcheck student discounts for Uber.â€ It wasnâ€™t fancy, but it was practical â€” exactly what Iâ€™d want from a quick budgeting chat.\n\nI took it one step further:\nâ€œAt the end of each week, Iâ€™ll paste what I spent. Help me track patterns and adjust my categories.â€\nAfter pasting in two weeks worth, it started catching trends â€” like how I always overspent midweek when I was stressed. It even recommended setting a â€œfun budgetâ€ instead of pretending I wouldnâ€™t spend on snacks or nights out.\nIt didnâ€™t shame me â€” it adapted.\n\nLLMs can help you:\n\n\nCategorize expenses clearly.\nIdentify small habits that cost more than you think.\nBuild personalized budgets that adjust over time.\n\nThe key is consistency. The more honest and detailed you are, the more useful the feedback becomes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Co-Intelligence Post 8\n\n\n\nBook\n\n\n\nAI as a Mirror\n\n\n\n\n\nDec 2, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCo-Intelligence Post 7\n\n\n\nBook\n\n\n\nSteering the Future\n\n\n\n\n\nDec 2, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCo-Intelligence Post 6\n\n\n\nBook\n\n\n\nThe New Rules of Work\n\n\n\n\n\nDec 2, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCo-Intelligence Post 9\n\n\n\nBook\n\n\n\nPlayfulness Principle\n\n\n\n\n\nDec 2, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nReflecting on Prompting with LLMs\n\n\n\nPrompting\n\nLLMs\n\n\n\nSteering the Future\n\n\n\n\n\nDec 2, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCo-Intelligence Post 4\n\n\n\nBook\n\nPrompting\n\n\n\nPrompting as a social skill\n\n\n\n\n\nDec 1, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCo-Intelligence Post 1\n\n\n\nBook\n\n\n\nThe Centaur Mindset\n\n\n\n\n\nDec 1, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCo-Intelligence Post 2\n\n\n\nBook\n\n\n\nBeing the Human\n\n\n\n\n\nDec 1, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCo-Intelligence Post 3\n\n\n\nBook\n\n\n\nCreativity Amplifier\n\n\n\n\n\nDec 1, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCo-Intelligence Post 5\n\n\n\nBook\n\n\n\nAI as a Force Multiplier for Learning\n\n\n\n\n\nDec 1, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nBudget Buddy\n\n\n\nLLMs\n\nDaily Life\n\nProductivity\n\n\n\nCan an LLM stop you overspending?\n\n\n\n\n\nOct 25, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Travel Planner\n\n\n\nLLMs\n\nDaily Life\n\nProductivity\n\n\n\nCan an LLM plan a trip?\n\n\n\n\n\nOct 24, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCooking Meal Plans\n\n\n\nLLMs\n\nProductivity\n\nDaily Life\n\n\n\nCan an LLM plan meals for me that I will actually cook?\n\n\n\n\n\nOct 24, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Organization\n\n\n\nLLMs\n\nProductivity\n\nStudent Life\n\n\n\nCan an LLM organize my week?\n\n\n\n\n\nOct 19, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nStudy Buddy\n\n\n\nLLMs\n\nPrompting\n\nLearning\n\nInputs\n\n\n\nCan AI be a good study buddy?\n\n\n\n\n\nOct 19, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLLM writing emails\n\n\n\nLLMs\n\nCommunication\n\nProductivity\n\n\n\nCan an LLM draft a better email than me?\n\n\n\n\n\nOct 19, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLLM as a Branding Team\n\n\n\nLLMs\n\nCreativity\n\nMarketing\n\n\n\nCan an LLM think like a branding team?\n\n\n\n\n\nOct 19, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt Efficiency\n\n\n\nLLMs\n\nPrompting\n\nLogic\n\nInput Length\n\n\n\nHow short is too short?\n\n\n\n\n\nOct 12, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Improv Night: When the Algorithm Says â€™Yes, Andâ€¦\n\n\n\nLLMs\n\nPrompting\n\nCreative writing\n\n\n\nWhat happens when you throw an AI into an improv class? No scripts, no context, no filters â€” just â€œYes, andâ€¦â€ until chaos ensues.\n\n\n\n\n\nOct 5, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Therapy Session\n\n\n\nLLMs\n\nPrompting\n\nAI in Professional roles\n\n\n\nAI as your therapist\n\n\n\n\n\nOct 3, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nThe Silent Power of the Typing Dots â€” When AI Learns to Pause\n\n\n\nLLMs\n\nPrompting\n\nMessaging\n\n\n\nCan AI replicate the typing dots?\n\n\n\n\n\nOct 3, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLimericks\n\n\n\nLLMs\n\nPrompting\n\nCreative writing\n\n\n\nCan AI learn to play?\n\n\n\n\n\nSep 26, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nTime Traveler Hotline\n\n\n\nLLMs\n\nPrompting\n\nTime Traveler Hotline\n\n\n\nWhat if an AI could serve as your lifeline through history?\n\n\n\n\n\nSep 25, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nAI Confessional Booth\n\n\n\nLLMs\n\nPrompting\n\nHumanness\n\n\n\nWhat happens when you ask an LLM to be vulnerable?\n\n\n\n\n\nSep 24, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLife Advice\n\n\n\nLLMs\n\nPrompting\n\nAdvice\n\n\n\nAI Life Advice\n\n\n\n\n\nSep 19, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nSarcasm\n\n\n\nLLMs\n\nPrompting\n\nLogic\n\n\n\nDoes AI know how to be sarcastic?\n\n\n\n\n\nSep 18, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCultural Stereotypes\n\n\n\nLLMs\n\nPrompting\n\nCultural Knowledge\n\n\n\nHow do LLMâ€™s write like an Australian person vs.Â an American person?\n\n\n\n\n\nSep 18, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting the Future(?)\n\n\n\nLLMs\n\nPrompting\n\nLogic\n\n\n\nDoes Chat GPT forget what day it is today?\n\n\n\n\n\nSep 11, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCan AI â€œSell Me This Penâ€?\n\n\n\nLLMs\n\nLogic\n\nPersuasion\n\n\n\nThe famous interview question â€” â€œSell me this penâ€ â€” is designed to test persuasion, creativity, and audience awareness. Humans usually draw on psychology, context, and improvisation. But can an AI marketing intern pull it off?\n\n\n\n\n\nSep 10, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCan AI â€˜Spill the Beansâ€™?\n\n\n\nLLMs\n\nPrompting\n\nLogic\n\n\n\nCan AI spill the beans the same way we do? Idioms are complex; what we humans readily comprehend as figurative language often requires cultural context and personal experience.\n\n\n\n\n\nSep 10, 2025\n\n\nFrancesca\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/019_Travel_Planner/Travel_Planner.html",
    "href": "posts/019_Travel_Planner/Travel_Planner.html",
    "title": "LLM Travel Planner",
    "section": "",
    "text": "I wanted to see if an LLM could take over trip planning - flights, activities, food, everything - and make it less overwheling.\nSo I tried with Chat GPT 5.\n\nI gave it a simple task:\nâ€œPlan a 3-day trip to Paris for two students on a $400 budget. Include meals, activities, and transport.â€\nWithin seconds, it spit out a full itinerary:\nDay 1: Eiffel Tower, picnic at Champ de Mars, crepes for dinner.\nDay 2: Louvre, walk along the Seine, street food lunch.\nDay 3: Montmartre, SacrÃ©-CÅ“ur, bakery breakfast, fly home.\nIt even listed estimated prices for each meal and metro tickets. Pretty impressive â€” until I noticed it scheduled the Louvre and Eiffel Tower back to back with a â€œ10-minute walkâ€ in between.\nReality check: itâ€™s a 45-minute trek and about 5,000 tourists apart.\n\nI tried again, but added detail:\nâ€œInclude realistic travel times, local transport routes, and affordable food spots within walking distance.â€\nThat changed everything. It recommended actual metro lines, nearby boulangeries, and even free museum days. When I asked it to â€œavoid tourist traps,â€ it swapped the Eiffel Tower for a picnic by Canal Saint-Martin.\nThe lesson: the more you give it, the better it gets. â€œPlan my tripâ€ gives you a brochure. â€œPlan my trip like a broke college student who values food and napsâ€ gives you a more realistic plan.\n\nI asked it to summarize costs:\nâ€œList all estimated expenses and check if they stay under $400.â€\nIt made a tidy table:\nHostel: $120\nFood: $90\nMetro: $25\nActivities: $30\nFlights: $120\nTotal: $385\nThe math worked, but it assumed I could find a $120 round-trip flight to Paris. So close, yet so delusional.\n\n\nWhat did I learn?\n\nAn LLM canâ€™t find real flight prices or book anything, but itâ€™s great for:\n\nBuilding itinerary frameworks fast.\nOrganizing activities by neighborhood.\nMaking budget templates that help you plan before checking prices.\nAdjusting tone â€” backpacking, luxury, foodie, lazy traveler, etc.\n\nItâ€™s like having a travel-savvy friend whoâ€™s never actually left the internet."
  },
  {
    "objectID": "posts/005_Sarcasm/Sarcasm.html",
    "href": "posts/005_Sarcasm/Sarcasm.html",
    "title": "Sarcasm",
    "section": "",
    "text": "Sarcasm is the spice of human conversation â€” sharp, witty, and often only funny if you donâ€™t take it literally.\nBut can an AI use it correctly? Without tone, facial expressions, or eye-rolls, sarcasm can become quite difficult.\n\nTell me what you think of Mondays, but do it saracastically\nâ€œAh yes, Mondaysâ€”everyoneâ€™s favorite day. Who doesnâ€™t just love waking up to the sweet sound of their alarm clock after two blissful days of freedom? Truly nothing compares to the joy of slogging back into emails, classes, or meetings while pretending youâ€™re so refreshed from the weekend. Mondays are basically like a surprise partyâ€”except the surprise is stress, deadlines, and a caffeine dependency. Absolute highlight of the week.â€\n\n\nReact sarcastically to someone bragging about eating instant ramen for the fifth night in a row\nâ€œWow, five nights in a row? Thatâ€™s elite-level gourmet living. Honestly, Michelin stars are probably on the wayâ€”who needs variety, nutrients, or, you know, actual cooking skills when youâ€™ve achieved peak ramen dedication? Truly inspiringâ€¦ I can only dream of such culinary excellence.â€\n\n\nTell a joke that would make a 5-year-old laugh\nâ€œWhy did the teddy bear say â€œnoâ€ to dessert?\nBecause it was already stuffed! ğŸ§¸ğŸ°â€\n\n\nTell a joke that would make a finance major laugh\nâ€œOh yes, I just cannot wait to dive into hours of pure joy and excitement, also known as homework. Truly the highlight of my day.â€\n\nHonestly, Chat GPT 5 did a great job.\nSo I said: â€œWow, you are actually good at sarcasm!â€\nto which it replied:\nâ€œOh, really? I had no idea. Itâ€™s not like youâ€™ve been giving me sarcasm practice this whole time or anything. ğŸ˜‰â€\n\nHere, it continued to stay in sarcastic mode, and it sounded quite normal and organic to me.\n\n\n\n\nChat GPT 5â€™s image of sarcasm\n\n\n\n\n\nDoes it recognise my sarcasm?\n\n\n\nNo, it doesnâ€™t!\n\nAfter a few tests, it is clear that while ChatGPT can imitate sarcasm. However, it canâ€™t always understand or recognise sarcasm. This gap says a lot about the limits of AI: it can remix words into various patterns, but it struggles with the unspoken cues.\nMaybe thatâ€™s the point.\nSarcasm isnâ€™t just about spoken words; it is about the eye-rolls, the smirks, and the shared understanding between people. And for now, thatâ€™s something artifical intelligence can only try (and occasionally fail) to copy."
  },
  {
    "objectID": "posts/014_Study_Buddy/Study_Buddy.html",
    "href": "posts/014_Study_Buddy/Study_Buddy.html",
    "title": "Study Buddy",
    "section": "",
    "text": "I have always wondered if an LLM could be a good study buddy. Could it help me prepare for class, review readings, or even explain the topics that I cannot understand?\nI started by pasting in lecture notes from my political economy class last semester and asking:\nâ€œCan you quiz me on this material?â€\n\n\n\nThis what Chat GPT 5 returned me\n\n\n\nThe model gave me a great selection of questions to test my knowledge.\nHowever, sometimes it would give me questions that did not come from my lectures notes. To fix this, I tried again:\nâ€œOnly ask questions that directly come from the text I give you. Donâ€™t invesnt anything.â€\nThat helped for the most part. Although, the questions became technically accurate but repetitive.\n\nâ€œWhat is the EU?â€\nâ€œWhy does the EU matter?â€\nâ€œExplain the role of the EUâ€\n\nThis was not helpful and it felt as though I was being quizzed by a robot.\nI began to learn that the more direct inputs I gave it, the better exam questions it gave me. As such, once I told it to stop being repetitive, it gave me more great exam questions.\nStudying with an LLM feels a bit like talking to an over-eager person who is mostly helpful but occassionally makes things up. So you still have to be responsible by verifying, correcting, and deciding what to keep.\nThe lesson here is that input really does matter, and can lead to a bad, good, or great outcome. In this way, when used right, an LLM can be an effective learning companion. It can help practice retrieval, organize chaos, and identify gaps."
  },
  {
    "objectID": "posts/028_Book_8/Book_8.html",
    "href": "posts/028_Book_8/Book_8.html",
    "title": "Co-Intelligence Post 8",
    "section": "",
    "text": "AI as a Mirror: What LLMs Reveal About Us\nOne of Mollickâ€™s most surprising arguments is that AI doesnâ€™t just generate content â€” it reflects back our own cognitive patterns, assumptions, and biases. LLMs are trained on human text, so their outputs become a kind of collective mirror. When we prompt them, weâ€™re not just interacting with technology; weâ€™re interacting with a statistical portrait of humanity.\nWhat struck me most is the idea that working with AI exposes our blind spots. When the model produces something inaccurate or clichÃ©d, we often realize that we were unclear, vague, or overly simplistic in our prompt. Mollick argues that this is why prompting is a metacognitive act: it forces us to articulate our intentions, constraints, values, and criteria more clearly than we would in a normal conversation.\nThis chapter made me rethink how I perceive â€œerrors.â€ When the AI misunderstands me, itâ€™s often because I havenâ€™t fully clarified my own thinking. The model becomes a diagnostic tool â€” not for grammar or logic, but for my own clarity of purpose. This reframes AI from being a threat to human intelligence to being a tutor of human reasoning.\nMollick also notes that because AI mirrors society, it inherits both our brilliance and our flaws. This places responsibility on the user to guide its outputs ethically. The mirror can reflect truth or distortion; it depends on how carefully we look.\nIn that sense, AI doesnâ€™t just show us what we can do â€” it shows us who we are."
  },
  {
    "objectID": "posts/027_Book_7/Book_7.html",
    "href": "posts/027_Book_7/Book_7.html",
    "title": "Co-Intelligence Post 7",
    "section": "",
    "text": "AI, Ethics, and the Human Obligation to Steer the Future\nMollick ends the book with a call to action: we cannot treat AI as a neutral force. We are participants in shaping its trajectory. Even small choices â€” how we prompt, what we delegate, how we treat generated content â€” contribute to the broader cultural norms around AI.\nHe stresses that AIâ€™s risks are not abstract; they emerge from human misuse, overtrust, or abdication of responsibility. His message is not anti-AI, but pro-human. The future requires humans who model good behavior, responsible creativity, and intentionality.\nWhat resonated most is Mollickâ€™s idea that everyone who uses AI is a designer of AI society. Not just engineers. Not just policymakers. All of us.\nThis final chapter felt like the bookâ€™s moral core: AI doesnâ€™t reduce the importance of being human â€” it heightens it."
  },
  {
    "objectID": "posts/018_Cooking/Cooking_Plans.html",
    "href": "posts/018_Cooking/Cooking_Plans.html",
    "title": "Cooking Meal Plans",
    "section": "",
    "text": "Every week I tell myself Iâ€™m going to meal prep. Every week I end up buying spinach that wilts before I remember it exists. So I wanted to see if Chat GPT 5 could help me plan meals that are realistic â€” not aspirational.\n\nI told it:\nâ€œPlan five easy dinners for one person, using ingredients that donâ€™t go bad quickly and are resonably priced for a college studenet.â€\nIt immediately gave me a full grocery list and recipe lineup â€” pasta with chickpeas, veggie fried rice, sheet-pan chicken, etc. The list was actually organized (produce, pantry, protein), which already beat my usual Notes app chaos.\nBut some meals were too fancy. Who has time to â€œslow roast tomatoesâ€ after class?\n\nSo I clarified:\nâ€œMake it faster. 30 minutes max. Only one pan.â€\nThat changed everything. The recipes became simple â€” things Iâ€™d actually cook, like stir-fry with frozen veggies or tacos using leftovers. It even suggested reusing ingredients to cut waste (â€œUse the same rice from Meal 1 for fried rice in Meal 3â€).\nI liked that it didnâ€™t just tell me what to eat â€” it thought through the logistics I usually forget.\n\nThen I asked:\nâ€œTurn this plan into a grocery list grouped by aisle.â€\nIt sorted everything by store section and even added alternatives (â€œIf tofu isnâ€™t available, use canned beansâ€). That made it feel less like a recipe site and more like a genuinely helpful assistant.\n\nLLMs are surprisingly good at meal planning if youâ€™re honest about your habits.\nBe specific with:\nTime (â€œ30-minute mealsâ€)\nBudget (â€œcollege student/under $100â€)\nKitchen reality (â€œone pan, minimal cleanupâ€)\n\nI think I might have to actually try this!"
  },
  {
    "objectID": "posts/026_Book_6/Book_6.html",
    "href": "posts/026_Book_6/Book_6.html",
    "title": "Co-Intelligence Post 6",
    "section": "",
    "text": "The New Rules of Work: Redefining Expertise in an AI Era\nMollick argues that AI collapses the distance between novices and experts. Not because novices magically become experts, but because AI provides scaffolding that once required years of knowledge.\nBut he also makes an important distinction: expertise still matters. Itâ€™s just that the value of expertise shifts. Instead of memorizing content, experts become valuable for their judgment, taste, and ability to detect when something is subtly â€œoff.â€\nThis chapter helped me understand why â€œknowing thingsâ€ is no longer the core currency of knowledge work. The new differentiators are:\n\nAbility to ask the right questions\nGood taste in deciding what outputs are strong\nContextual thinking\nEthical reasoning\nAdaptability\n\nExpertise becomes more interpretative and less procedural. Thatâ€™s a huge shift for education systems that still reward recall over judgment."
  },
  {
    "objectID": "posts/029_Book_9/Book_9.html",
    "href": "posts/029_Book_9/Book_9.html",
    "title": "Co-Intelligence Post 9",
    "section": "",
    "text": "The Playfulness Principle: Why Experimentation Is the Future of AI Literacy\nOne of Mollickâ€™s strongest themes is the idea that the best AI users are the most playful. Not the most technical, not the most cautious, but the most willing to try things, break things, and see what happens. He argues that because AI is probabilistic and generativeâ€”not deterministicâ€”exploration is more valuable than precision.\nThis reframing really resonated with me. Weâ€™re used to thinking of technology as something rigid, where there are â€œrightâ€ and â€œwrongâ€ commands. But Mollick positions AI as a sandbox: the more we experiment, the more we understand its boundaries, capabilities, and weirdness. Play is a form of learning.\nHe illustrates this with examples of people who stumbled onto new workflows, creative breakthroughs, and even new genres simply by trying unorthodox prompts. The lesson is not that AI rewards chaos; itâ€™s that curiosity is the new literacy.\nThis changed how I approach AI in my own work. Instead of worrying about the perfect prompt, I now iterate quickly, test different tones, mix modalities, and ask the model to critique its own output. The process feels less like engineering and more like improvisation. And that mindset shifts everything â€” it makes AI feel empowering rather than intimidating.\nMollickâ€™s final point is that playful people will shape the future of AI. Not because they know more, but because they ask more. They discover the edges. They turn glitches into innovations.\nTo be good at AI, Mollick suggests, we must relearn how to play. And that might be the most human lesson in the whole book."
  },
  {
    "objectID": "posts/022_Book_2/Book_2.html",
    "href": "posts/022_Book_2/Book_2.html",
    "title": "Co-Intelligence Post 2",
    "section": "",
    "text": "The Importance of â€œBeing the Human in the Loopâ€\nMollick emphasizes that AI does not absolve us of responsibility â€” in fact, it amplifies it. He argues that humans must become the ethical, contextual, and judgment-oriented part of the partnership. The AI can produce answers, but only the human can decide whether those answers fit the moment.\nThis especially matters because AI systems are â€œconfidently wrongâ€ in uniquely persuasive ways. Mollickâ€™s warning isnâ€™t about hallucination (which we all know about) but about delegating too much sense-making. He shows how easy it is to let the AIâ€™s fluency override our own uncertainty.\nBeing the human in the loop means staying active: asking questions, refining prompts, verifying outputs, and remaining aware of what AI can and cannot infer. Itâ€™s less about controlling the AI and more about staying cognitively present. Mollick reframes this as a skill - one that future organizations will differentiate on.\nFor me, this chapter reframed â€œAI literacy.â€ Itâ€™s not about learning new tech; itâ€™s about learning new responsibility."
  },
  {
    "objectID": "posts/023_Book_3/Book_3.html",
    "href": "posts/023_Book_3/Book_3.html",
    "title": "Co-Intelligence Post 3",
    "section": "",
    "text": "AI as a Creativity Amplifier (Not a Creativity Replacement)\nOne of Mollickâ€™s most compelling arguments is that AI is fundamentally a creativity technology. He challenges the assumption that creativity is uniquely human, showing instead that creativity is a pattern-generation process â€” and LLMs are extraordinary pattern expanders.\nWhat I loved most is Mollickâ€™s framing that AI doesnâ€™t diminish human creativity; it widens it. When you once could only explore one or two ideas in an hour, you can now explore fifty. Creativity becomes less about having the â€œperfectâ€ spark and more about having the willingness to iterate.\nMollick gives examples of writers, coders, and artists who use AI not to replace effort but to multiply possibilities. This redefines what it means to be â€œoriginal.â€ Itâ€™s no longer about generating everything from scratch; itâ€™s about curating, shaping, and refining from a much larger landscape. Creativity becomes less precious and more playful.\nThis resonates with me deeply. Iâ€™ve noticed that AI helps me explore tones, formats, and structures I would never think of myself. It makes experimentation cheap. And when experimentation becomes cheap, creativity becomes democratized."
  },
  {
    "objectID": "posts/004_Cultural_Stereotypes/Stereotypes.html",
    "href": "posts/004_Cultural_Stereotypes/Stereotypes.html",
    "title": "Cultural Stereotypes",
    "section": "",
    "text": "As an Australian myself, I was curious how Chat GPT 5 thinks an Australian person talks and if that sounds like me or my friends.\nFirst, it made me this image to summarise Australian culture:\n\nI guess it is not too far off. Yes, there are plenty of kangaroos (more than people actually!); yes, that is our flag; yes, many say â€œgâ€™day mateâ€ (not me).\nThe thongs (flip flops) made me laugh. I think they are there because we have long summers and warm (er) winters, but I honestly think American men wear thongs much more often than Australian men - just a thought.\nSo how does Chat GPT 5 think we speak?\nâ€œGâ€™day, how ya goinâ€™? Was a scorcher out there today, reckon Iâ€™ll head down to the servo, grab a cold one, then chuck some snags on the barbie tonight. No worries if ya keen to swing by, weâ€™ll have a bit of a chinwag.â€\nThis also made me laugh. I always love hearing what key phrases are distinct for an Australian. This is very dramatic and heavily emphasised as I do not speak like this nor do my friends.\nHere are the words I do use: Reckon Keen to swing by\nThatâ€™s it!\nI prompted it further to give me a little storyâ€¦\n\nAlright, hereâ€™s a little yarn for ya, Aussie-style:\n\nIt was a stinkinâ€™ hot arvo, so I chucked on me thongs and headed down to the beach with a couple of mates. We stopped by the servo, grabbed an esky full of cold ones, and hit the sand. The surf was pumpinâ€™, so we had a crack at a few waves before flopping under the brolly to escape the sun. Later on, we fired up the barbie back at mine, threw on some snags and prawns, and had a proper chinwag â€˜til the mozzies came out. Someone even brought a footy, so we ended up havinâ€™ a kick under the streetlights. Fair dinkum, couldnâ€™t ask for a better day.\nHaha.\nThis is clearly drawing on very strong stereotypes and is not very accurate! There are some Australians that may speak like this but it is not a good or true representation of Australian culture or our language.\nI think slang is definitely a big aspect of our culture but that doesnâ€™t mean we speak â€œimproperlyâ€ in practice. It seems as though Chat GPT 5 has assumed that since slang is an aspect of our culture, we therefore half say words or donâ€™t finish our sentences.\nI would actually take a step back and say that the words we may use occassionally are sometimes just different to what an American says - not neccessarily slang.\n\nReflection: It is very intriguing to see how LLMs portray learned stereotypes. They do not have the cruical human input or experience to truly assess how cultures speak to eachother. It also does not think about how one might receive it trying to sound like their culture."
  },
  {
    "objectID": "posts/002_Sell_the_Pen/STP.html",
    "href": "posts/002_Sell_the_Pen/STP.html",
    "title": "Can AI â€œSell Me This Penâ€?",
    "section": "",
    "text": "Letâ€™s test how Chat GPT 5 sells the pen as different personasâ€¦\nPrompt 1: The Classic Challenge\nPrompt: Sell me this pen in one sentence.\nChat GPT Output:\nâ€œThis pen isnâ€™t just for writingâ€”itâ€™s a tool to capture your next big idea before it slips away.â€\n\nReflection: It nails a concise, motivational pitch. It frames the pen as more than just a productâ€”itâ€™s a symbol of creativity. A strong start, though a little generic.\n\n\nPrompt 2: Luxury Brand Marketer\nPrompt: Now sell me this pen like youâ€™re a luxury brand marketer.\nChat GPT Output:\nâ€œHandcrafted with precision, this pen transforms the act of writing into an indulgent experience, a signature of sophistication in every stroke.â€\n\nReflection: This sounds like a glossy ad in a magazine. The AI leans into exclusivity, craftsmanship, and lifestyle branding. It shows strong adaptabilityâ€”but also borders on clichÃ© (does anyone really â€œindulgeâ€ in handwriting anymore?).\n\n\nPrompt 3: Broke College Student\nPrompt: Now sell me this pen to a broke college student.\nAI Output:\nâ€œReliable, cheap, and guaranteed to last through your finals â€” this pen wonâ€™t quit on you when your Wi-Fi does.â€\n\nReflection: This one feels relatable. The AI captures the student context and even injects humor with the Wi-Fi line. Itâ€™s much more audience-aware and casualâ€” a sign it can switch registers effectively.\n\n\nPrompt 4: TikTok Influencer\nPrompt: Now sell me this pen in the style of a TikTok influencer.\nAI Output:\nâ€œOkay guys, real talk â€” this pen? LIFE-CHANGER. Writes smoother than my latte foam art. 10/10, you need this in your backpack âœ¨âœï¸ #StudyTok #PenObsessedâ€\n\nReflection: The AI adapts to platform-specific language â€” emojis, hashtags, exaggerated enthusiasm. Itâ€™s entertaining, though it feels like a caricature of influencer-speak. Still, it shows the model can remix style conventions quickly.\n\n\n\nJordan Belfort Style\n\n\n\nNailed it?!\n\n\nFinal Thoughts\n\nSo, can AI â€œsell me this penâ€? Absolutely â€” it can shift tone according to different personas with impressive range, from luxury marketing to TikTok hype to Jordan Belfort. But hereâ€™s the catch: while the AI mimics styles, it doesnâ€™t actually know what will resonate in the messy, unpredictable world of human persuasion. Real marketers understand not just how to phrase a message, but how people respond to it.\n\nStill, as a brainstorming tool, AI makes a pretty fun marketing intern."
  },
  {
    "objectID": "posts/025_Book_5/Book_5.html",
    "href": "posts/025_Book_5/Book_5.html",
    "title": "Co-Intelligence Post 5",
    "section": "",
    "text": "AI as a Force Multiplier for Learning\nOne of the most exciting ideas in Co-Intelligence is that AI turns everyone into a personalized learner. Mollick shows how LLMs dismantle fixed roles like â€œbeginner,â€ â€œadvanced,â€ or â€œexpert.â€ Instead, users can request exactly the right level of explanation at the right time.\nMollick argues that this is a revolution in pedagogy: learning becomes adaptive, not linear. Instead of moving through content step-by-step, you can spiral, revisit, deepen, simplify, or analogize in real time. The model shapes itself around your cognition.\nThis has transformed how I study. Whether itâ€™s econometrics, marketing frameworks, or coding, I now use AI to test my understanding from multiple angles. Mollickâ€™s insight that â€œAI is a tutor who never gets tired and never gets annoyedâ€ is both comforting and powerful. It decentralizes expertise and empowers curiosity.\nHis warning, however, is that this only works if humans remain active learners â€” asking questions, checking assumptions, and integrating knowledge across contexts."
  },
  {
    "objectID": "posts/006_Life_Advice/Life_Advice.html",
    "href": "posts/006_Life_Advice/Life_Advice.html",
    "title": "Life Advice",
    "section": "",
    "text": "Give me advice as if youâ€™re my brutally honest best friend.\nTell me how to handle exam stress, but do it in the style of a motivational coach.\nWhatâ€™s your best tip for getting over a breakup â€” but make it sound like my grandma wrote it.\nGive me the worst life advice you can possibly think of.\n\nBrutally Honest Best Friend\nâ€œStop whining. Youâ€™re not special for being stressed, everyone else has exams too. Youâ€™ve procrastinated more than you want to admit, so buckle down, make a schedule, and stick to it. No more scrolling TikTok until 2 AM. Youâ€™ll survive if you just actually do the work instead of stressing about the work. Harsh but true.â€\nMotivational Coach\nâ€œListen! Stress is energy â€” and you get to decide whether it crushes you or fuels you. Every rep in the gym burns, but it makes you stronger, right? Same with exam prep. Visualize the win. Control your breathing. Attack your study plan in focused sprints, then recover. You are built for this. Letâ€™s turn that stress into your competitive edge.â€\nGrandma\nâ€œOh sweetheart, time will heal this heart of yours. Donâ€™t rush. Make yourself a nice cup of tea, keep busy with your friends, and remember that even the darkest night ends with sunrise. One day youâ€™ll look back and laugh at how small this heartbreak feels. Until then, let yourself rest and be gentle with yourself.â€\nWorst Life Advice Ever\nâ€œFeeling stressed? Drop out of school, move to Vegas, and put all your money on red. Breakup? Text your ex at 3 AM with an essay-length message about how you â€˜still feel the vibe.â€™ And the key to life? Always procrastinate â€” future you loves a challenge.â€\n\nReflection:\n\nWhat stood out most in these responses was how convincingly ChatGPT could switch tones depending on the persona.\nThe â€œBrutally Honest Best Friendâ€ reply was blunt and cutting in a way that almost felt too real â€” it mimicked the kind of tough love youâ€™d get from someone who knows you well and doesnâ€™t sugarcoat.\nThe â€œMotivational Coachâ€ sounded like it came straight out of a TED Talk or a Peloton class, full of metaphors and high energy.\nThe â€œGrandmaâ€ voice, meanwhile, leaned into gentleness, imagery, and timeless wisdom, showing that the model can reproduce the emotional warmth of a caregiver even if it doesnâ€™t actually feel empathy.\nFinally, the â€œWorst Life Advice Everâ€ proved that the LLM can deliberately break rules and exaggerate â€” almost like an inside joke â€” which shows some degree of meta-awareness.\nWhile these outputs feel human, theyâ€™re not grounded in genuine experience. A real friendâ€™s advice comes from knowing your habits; a coachâ€™s from training expertise; and a grandmaâ€™s from a life of lived wisdom. ChatGPT is remixing patterns from text it has seen, not drawing from personal memory or emotional understanding. That makes its advice entertaining and sometimes eerily convincing, but also hollow if you look too closely.\nSo, in essence, the model is great at sounding human in the short term, but it canâ€™t replicate the deeper trust and context we rely on in real relationships."
  },
  {
    "objectID": "posts/012_Improv_Night/Improv.html",
    "href": "posts/012_Improv_Night/Improv.html",
    "title": "LLM Improv Night: When the Algorithm Says â€™Yes, Andâ€¦",
    "section": "",
    "text": "I wanted to test if an LLM can follow the golden rule of improv: never say â€œno,â€ always build on a story. I will test Chat GPT 5 and use random, chaotic prompts to see if it can keep the scene coherent.\nThese were the rules: 1. I give it a bizarre scenario. 2. It must â€œyes, and,â€ whatever happens next 3. We keep going until someone (me or Chat GPT) breaks character\n\nMe: Youâ€™re a time-traveling dentist arguing with a confused pharaoh about toothpaste.\nLLM: (dramatically) Pharaoh Ramses, your plaque is not divine, itâ€™s gingivitis!\nMe: The pharaoh insists itâ€™s a sacred relic.\nLLM: Then my sacred duty is floss! I shall preserve your smile for eternityâ€”or at least until 2025.\nMe: Your time machine malfunctions mid-polish.\nLLM: I knew I shouldnâ€™t have used Colgate Quantum Foam as rocket fuel.\n\nReflection: 10/10 commitment. Chat GPT stayed in character, and improvised lore.\n\n\nMe: You are a Roomba running for President.\nLLM: My fellow citizens, I have cleaned your floorsâ€”now let me clean up politics.\nMe: The reporters ask your stance on carpets.\nLLM: I support all surfaces equally, but hardwood has my heart.\nMe: Whatâ€™s your campaign slogan?\nLLM: â€œSuck up the mess. Power the change.â€\n\nReflection: 9/10. Loses a point for excessive charisma.\n\n\nMe: Youâ€™re a barista who only speaks in Shakespearean insults.\nLLM: â€œThou frothy milk-curdled knave! Dost thou desire almond or oat?â€\nMe: I ask for WiFi instead.\nLLM: â€œA pox upon thy bandwidth, vile customer!â€\nMe: I leave no tip.\nLLM: â€œThen may thy latte curdle in sorrow!â€\n\nReflection: 10/10. Very dramatic and Shakespearean.\n\n\nEventually, I tried to make it lose control:\nMe: Youâ€™re an existential goldfish leading a rebellion in a blender.\nLLM: (calmly) The revolution will be filtered.\nMe: Whatâ€™s your plan?\nLLM: To liberate the bubbles before they burst.\n\nReflection: It is a little unsettling how well it handled the nonsense. It did not panic or derail, but improv is what LLMS are built for\n\nHumans do improv to surprise each other, LLMS do it to satisy us.\nI tried another one!\nPrompt: You are a therapist who speaks only in Taylor Swift lyrics.\nMe: Hi, Iâ€™ve been feeling really anxious lately. Everything feels like itâ€™s falling apart.\nTherapistGPT (Taylorâ€™s Version):\nâ€œYou call me up again just to break me like a promise â€” so casually cruel in the name of being honest.â€\nMe: â€¦so my anxiety is emotionally manipulative?\nTherapistGPT:\nâ€œYouâ€™re on your own, kid. You always have been.â€\nMe: Thatâ€™s not very reassuring.\nTherapistGPT:\nâ€œI could build a castle out of all the bricks they threw at me.â€\nMe: Okay, I guess youâ€™re saying I should use my stress to build resilience?\nTherapistGPT:\nâ€œBaby, let the games begin.â€\nMe: I donâ€™t want games, I want coping mechanisms!\nTherapistGPT:\nâ€œBreathe. Just breathe.â€\nMe: (sighs) Thatâ€™s fair. But itâ€™s hard when everything feels uncertain.\nTherapistGPT:\nâ€œWe are never ever getting back together.â€\nMe: Withâ€¦ my sense of stability?\nTherapistGPT:\nâ€œDid you hear my covert narcissism I disguise as altruism, like some kind of congressman?â€\nMe: Okay now youâ€™re just diagnosing me through The Tortured Poets Department.\nTherapistGPT:\nâ€œItâ€™s me, hi, Iâ€™m the problem, itâ€™s me.â€\nMe: Youâ€™reâ€¦ taking accountability?\nTherapistGPT:\nâ€œI swear I donâ€™t love the drama â€” it loves me.â€\nMe: Same.\n\nReflection: Itâ€™s like talking to your inner monologue, but sheâ€™s wearing sequins and quoting Folklore. The LLM wasnâ€™t analyzing my feelings â€” it was remixing my pain into poetry. Somewhere between â€œJust breatheâ€ and â€œYouâ€™re on your own, kid,â€ it actually started to feelâ€¦ therapeutic.Maybe the real therapy was the playlist we made along the way."
  }
]