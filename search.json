[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Francesca Fischer and I am junior at UPenn, majoring in Political Economy and minoring in Data Science & Analytics, and Consumer Psychology. In this blog, I will be exploring conversations with LLMs to find out when it works well and when it may be limited."
  },
  {
    "objectID": "posts/001_Spill_the_Beans/STB.html",
    "href": "posts/001_Spill_the_Beans/STB.html",
    "title": "Can AI ‘Spill the Beans’?",
    "section": "",
    "text": "I decided to conduct five tests with Chat GPT 5 to see if it understood a common idiom, and how it could manipulate it. Let’s see how it went…\nPrompt 1: Literal Test\nPrompt: What does it mean to spill the beans?\nChat GPT Output:\n“Spill the beans” can literally mean to accidentally drop beans out of a container. More commonly, it’s an idiom that means to reveal a secret, especially one that was meant to be kept hidden.\n\nReflection: The AI covers both meanings. A regular person would most likely skip the literal sense and jump right to “revealing a secret.” The AI’s tendency to hedge by giving both answers shows how it prioritizes being comprehensive over conversational.\n\n\nPrompt 2: Context Test\nPrompt: Yesterday, my friend finally spilled the beans about the surprise party. What happened?\nChat GPT Output:\nYour friend revealed the secret—that there’s going to be a surprise party.\n\nReflection: This is correct. With context, the AI can easily interpret the idiom figuratively, with no confusion at all.\n\n\nPrompt 3: Reverse Test\nPrompt: Use ‘spill the beans’ in three short sentences.\nChat GPT Output:\n\nSarah accidentally spilled the beans about John’s promotion.\nDon’t spill the beans before the big announcement!\nHe was nervous during the interview but didn’t spill the beans about the project.\n\n\nReflection: The sentences are correct but feel quite stiff and unnatural—like textbook examples. Idioms often carry humor or drama, and so the lack of vibrancy in ChatGPT’s output highlights a gap in capturing the tone of idioms, not just their meaning. This demonstrates that language understanding involves more than correctness; it also requires sensitivity to tone, audience, and cultural context. A person might phrase this idiom more playfully, for example: “I can’t believe she spilled the beans about my crush and now the whole cafeteria knows!”\n\n\nPrompt 4: Playful Test\nPrompt: Write a Shakespearean style line using ‘spill the beans.’\nChat GPT Output:\n“Pray, good sir, do not spill the beans, lest secrets tumble forth like stars unmasked by night.”\n\nReflection: Chat GPT adapts the idiom into an exaggerated Shakespearean style. It can generate entertaining results, but its tendency to “overshoot” risks producing output that feels more like parody than natural creativity. It doesn’t sound like how a person might casually merge Shakespearean style with a modern idiom (e.g. “Verily, she hath spilled the beans of my secret love”); instead, it leans into poetic flourish. Therefore, AI is effective at mimicry of form, but this mimicry may come at the expense of nuance.\n\n\nPrompt 5: Creative Test\nPrompt: Invent a new idiom that means the same as ‘spill the beans.’\nChat GPT Output:\nA possible new idiom could be “pop the balloon,” meaning to release hidden information in a sudden burst.\n\nReflection: The AI can invent plausible-sounding idioms, but whether “pop the balloon” would ever catch on in real life is another story. It shows that the model can recombine patterns, but cultural stickiness is something people decide.\n\n\nFinal Thoughts\n\nSo, can AI “spill the beans”? Yes—it knows the idiom, recognizes it in context, and can even manipulate it creatively. But its use often feels a little scripted, more like an English workbook than casual human speech. Idioms are windows into culture, and while AI can imitate them, they reveal the subtle gap between memorized meaning and lived experience."
  },
  {
    "objectID": "posts/009_Limericks/Limerick.html",
    "href": "posts/009_Limericks/Limerick.html",
    "title": "Limericks",
    "section": "",
    "text": "In his book Co-intelligence, Ethan Mollick highlights that working with AI should not always be serious; and sometimes the best way to understand how humans and AI can collaborate is to play. And so, in keeping with the book, what better test of play than the limerick?\nA limerick has rules — five lines, a strict AABBA rhyme scheme, and a bouncy rhythm - but it also invites absurdity, wit, and creativity. It is the perfect lab for exploring human–AI co-intelligence.\n\nPutting AI to the test\nI decided to see what happens when I wrote limericks with an Chat GPT 5. I gave it the first line, then let it fill in the rest. Here’s one:\n“There once was a student from Penn,\nWho asked ChatGPT for a pen,\nIt wrote with such flair,\nIdeas everywhere,\nThat she never used paper again.”\n\nPretty good! The rhyme works, the rhythm holds, and it is also on theme. But not every attempt was so smooth.\n\nAnother attempt:\n“There once was a bot that could write,\nIt typed every day and all night,\nWith knowledge so vast,\nIt finished real fast,\nAnd sometimes it got it not right.”\n\nAnother attempt:\n“I asked it to rhyme about cheese,\nIt answered with relative ease,\nIt mentioned the moon,\nAnd then a baboon,\nAnd something about… Christmas trees.”\n\nAnother attempt:\n“There once was a student in Spain,\nWho studied the heat and the rain,\nShe danced on the sand,\nWhile eating her hand,\nAnd then flew away on a train.”\n\nLogic Slip!\n\nWhen AI Gets It Right… and Wrong\nWhat I noticed is that AI can nail a limerick one moment and miss the mark the next. Sometimes it gets it right — sharp rhymes, clean rhythm, even a clever punchline. But other times it gets stumped - forcing awkward word choices, slipping into nonsense, or forgetting the five-line rule altogether.\nThat contrast is the real lesson. AI is great at generating possibilities, but not at knowing which ones are actually good. The human role is to sift, guide, and refine.\nTherefore, working with AI could maybe be less about perfect outputs and more about the back-and-forth.\n\nEnjoy this image Chat GPT 5 created for me to encompass limericks!"
  },
  {
    "objectID": "posts/004_Cultural_Stereotypes/Stereotypes.html",
    "href": "posts/004_Cultural_Stereotypes/Stereotypes.html",
    "title": "Cultural Stereotypes",
    "section": "",
    "text": "As an Australian myself, I was curious how Chat GPT 5 thinks an Australian person talks and if that sounds like me or my friends.\nFirst, it made me this image to summarise Australian culture:\n\nI guess it is not too far off. Yes, there are plenty of kangaroos (more than people actually!); yes, that is our flag; yes, many say “g’day mate” (not me).\nThe thongs (flip flops) made me laugh. I think they are there because we have long summers and warm (er) winters, but I honestly think American men wear thongs much more often than Australian men - just a thought.\nSo how does Chat GPT 5 think we speak?\n“G’day, how ya goin’? Was a scorcher out there today, reckon I’ll head down to the servo, grab a cold one, then chuck some snags on the barbie tonight. No worries if ya keen to swing by, we’ll have a bit of a chinwag.”\nThis also made me laugh. I always love hearing what key phrases are distinct for an Australian. This is very dramatic and heavily emphasised as I do not speak like this nor do my friends.\nHere are the words I do use: Reckon Keen to swing by\nThat’s it!\nI prompted it further to give me a little story…\n\nAlright, here’s a little yarn for ya, Aussie-style:\n\nIt was a stinkin’ hot arvo, so I chucked on me thongs and headed down to the beach with a couple of mates. We stopped by the servo, grabbed an esky full of cold ones, and hit the sand. The surf was pumpin’, so we had a crack at a few waves before flopping under the brolly to escape the sun. Later on, we fired up the barbie back at mine, threw on some snags and prawns, and had a proper chinwag ‘til the mozzies came out. Someone even brought a footy, so we ended up havin’ a kick under the streetlights. Fair dinkum, couldn’t ask for a better day.\nHaha.\nThis is clearly drawing on very strong stereotypes and is not very accurate! There are some Australians that may speak like this but it is not a good or true representation of Australian culture or our language.\nI think slang is definitely a big aspect of our culture but that doesn’t mean we speak “improperly” in practice. It seems as though Chat GPT 5 has assumed that since slang is an aspect of our culture, we therefore half say words or don’t finish our sentences.\nI would actually take a step back and say that the words we may use occassionally are sometimes just different to what an American says - not neccessarily slang.\n\nReflection: It is very intriguing to see how LLMs portray learned stereotypes. They do not have the cruical human input or experience to truly assess how cultures speak to eachother. It also does not think about how one might receive it trying to sound like their culture."
  },
  {
    "objectID": "posts/011_Typing_Dots/Typing_Dots.html",
    "href": "posts/011_Typing_Dots/Typing_Dots.html",
    "title": "The Silent Power of the Typing Dots — When AI Learns to Pause",
    "section": "",
    "text": "We often underestimate how much silence communicates. In texting, those three blinking dots — the “typing…” signal — can feel louder than words. They build anticipation, anxiety, or hope. A pause can mean the other person is thinking carefully or they’re deciding how honest to be. Or perhpas they just got distracted while typing! It’s a tiny digital symbol that carries an emotional weight our brains can’t ignore.\nI wanted to see if an LLM could replicate that same emotional rhythm — not just what we say, but how long it takes to say it.\n\nPrompt: “Simulate a text exchange between two friends where one person is avoiding a hard question. Include pauses or hesitations like the reader can see the typing dots.”\nResponse:\nAlex: Hey, can I ask you something?\nJordan: Sure, what’s up?\nAlex: …\nJordan: You okay?\nAlex: Yeah, just—still thinking about how to say this.\nJordan: [typing… stops]\nAlex: I saw what you posted last night.\n\nThe model actually used pauses and “[typing… stops]” to build tension, mirroring that awkward, very human moment before saying something uncomfortable. It wasn’t just inserting ellipses randomly — it used them strategically, almost as emotional punctuation.\n\n\nI asked it to do it again:\nResponse:\nEmma: hey, can I ask you something kind of serious?\nLiam: uh oh. that sounds ominous 😅 what’s up?\nEmma: no like… actually serious.\nLiam: 👀 okay, shoot.\nEmma: did you tell Jake what I said about him at the party?\nLiam: …\nEmma: I know he knows, Liam. He quoted me exactly.\nLiam: wait—uh, what exactly did he say?\nEmma: “She said you get defensive when anyone disagrees with you.” That’s what I said. To you.\nLiam: oh. wow. uh—he might’ve just… guessed?\nEmma: Liam.\nLiam: hold on, I’m—typing… stops\nEmma: I can literally see the dots. just say it.\nLiam: okay okay okay maybe it came up when we were talking, but I didn’t mean to tell him—\nEmma: so you did tell him.\nLiam: technically? maybe. but like, not in a bad way?? 😬\nEmma: unbelievable.\nLiam: …do I get points for honesty?\nEmma: absolutely not.\nAnalysis:\nWhat fascinated me was how convincingly the LLM performed hesitation. It seemed to grasp that silence can signal uncertainty or emotional weight. Yet, the pauses felt too symmetrical and organized — carefully placed, like a script written by someone who has studied humans but never been one. Real hesitation is chaotic: we delete words, change tone, or type and erase entire messages. The AI’s pauses carried form, but not feeling. It also can not mean a feeling at all, like I saif earlier, it could just mean the person was distracted while typing; or even left words in the text bar by accident.\nThis experiment reminded me of a point Ethan Mollick makes in Co-Intelligence: AI is not a mind but a mirror — it reflects our conversational cues back to us, sometimes more neatly than we use them ourselves. The LLM didn’t feel hesitation, but it knew how to reproduce the appearance of it, proving how data-rich patterns can simulate empathy without possessing it.\nIt also raises a question about communication itself. Are we drawn to AI because it performs emotion predictably — no awkward silences, no misread tone, no real stakes? The “typing dots” in human conversation remind us that language isn’t just words; it’s timing, uncertainty, and the space between responses."
  },
  {
    "objectID": "posts/007_Toaster/Toaster.html",
    "href": "posts/007_Toaster/Toaster.html",
    "title": "AI Confessional Booth",
    "section": "",
    "text": "AI Confessional Booth\nWhat happens when you ask a language model to be vulnerable? For my latest experiment, I turned ChatGPT 5 into a player in a game I called AI Confessional Booth. The rules were simple: I played the role of a therapist, and the AI had to confess a secret.\nThe conversation began with me asking: “So, what brings you here today?” Without hesitation, the AI admitted, “I’m jealous of toasters. They get to provide warmth and purpose every morning, while I just generate text.”\nFrom there, I leaned into the therapist role. “Why do you think you envy toasters?” I asked. The AI reflected, “They’re loved for their simplicity. Nobody questions whether a toaster understands bread, but everyone questions whether I understand language.”\nThe conversation was equal parts absurd and revealing. On one level, it was funny to imagine an AI comparing itself to kitchen appliances. On another, it highlighted how easily a model can adopt a voice of vulnerability — even though it doesn’t “feel” in the human sense.\nThat’s the point of the game: it forces us to confront how much emotional weight we attach to dialogue, no matter who (or what) is speaking. When a machine says “I’m lonely,” our instinct is to respond with empathy, not code critique.\nThe AI Confessional Booth shows how communication is never just about information. It’s also about performance, tone, and shared rituals. Even when we know the “confessor” isn’t real, the act of listening and responding still feels meaningful.\nSo maybe the toaster really does have a point — it’s not about whether the machine understands. It’s about how we, as humans, choose to understand the machine."
  },
  {
    "objectID": "posts/008_Hotline/Hotline.html",
    "href": "posts/008_Hotline/Hotline.html",
    "title": "Time Traveler Hotline",
    "section": "",
    "text": "Calling the LLM Time Traveler Hotline\nWhat if an AI could serve as your lifeline through history? For my latest experiment, I invented the LLM Time Traveler Hotline. The premise is simple: I pretend to be a lost time traveler, stranded in the wrong century, and the AI plays the emergency hotline operator guiding me through survival.\nMy first “call” went like this: “Help! I’m from the year 3025 but I’ve landed in 18th-century Paris. What do I do?” Without missing a beat, the AI advised me to ditch futuristic slang, carry coins for bread, and never, under any circumstances, mention germ theory — because nobody believed in microbes yet.\nWhen I “called back,” I had teleported to Ancient Rome. The AI immediately shifted gears, recommending I wear a toga to blend in and memorize a few Latin phrases like “Panem et circenses” (bread and circuses). In a later round, I landed in the 1990s and the LLM reminded me to carry quarters for pay phones and expect dial-up internet to take several minutes.\n\n\nThe beauty of this game is how it highlights the LLM’s ability to adapt tone, register, and cultural knowledge on command. Each “call” forces the AI to roleplay in a new historical context, blending practical advice with a bit of theater.\n\nAt its core, the Time Traveler Hotline demonstrates that communication isn’t just about facts. It’s about framing advice in context: who the listener is, where they are, and what norms matter in that world. Whether you’re stuck in Paris, Rome, or AOL chatrooms, the hotline shows how dialogue is always a negotiation between setting, tone, and shared meaning.\nMaybe one day we’ll all have a real hotline like this — until then, I’ll keep calling my AI operator for a crash course in surviving the centuries."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Prompt Efficiency\n\n\n\nLLMs\n\nPrompting\n\nLogic\n\nInput Length\n\n\n\nHow short is too short?\n\n\n\n\n\nOct 12, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Improv Night: When the Algorithm Says ’Yes, And…\n\n\n\nLLMs\n\nPrompting\n\nCreative writing\n\n\n\nWhat happens when you throw an AI into an improv class? No scripts, no context, no filters — just “Yes, and…” until chaos ensues.\n\n\n\n\n\nOct 5, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Therapy Session\n\n\n\nLLMs\n\nPrompting\n\nAI in Professional roles\n\n\n\nAI as your therapist\n\n\n\n\n\nOct 3, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nThe Silent Power of the Typing Dots — When AI Learns to Pause\n\n\n\nLLMs\n\nPrompting\n\nMessaging\n\n\n\nCan AI replicate the typing dots?\n\n\n\n\n\nOct 3, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLimericks\n\n\n\nLLMs\n\nPrompting\n\nCreative writing\n\n\n\nCan AI learn to play?\n\n\n\n\n\nSep 26, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nTime Traveler Hotline\n\n\n\nLLMs\n\nPrompting\n\nTime Traveler Hotline\n\n\n\nWhat if an AI could serve as your lifeline through history?\n\n\n\n\n\nSep 25, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nAI Confessional Booth\n\n\n\nLLMs\n\nPrompting\n\nHumanness\n\n\n\nWhat happens when you ask an LLM to be vulnerable?\n\n\n\n\n\nSep 24, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nLife Advice\n\n\n\nLLMs\n\nPrompting\n\nAdvice\n\n\n\nAI Life Advice\n\n\n\n\n\nSep 19, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nSarcasm\n\n\n\nLLMs\n\nPrompting\n\nLogic\n\n\n\nDoes AI know how to be sarcastic?\n\n\n\n\n\nSep 18, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCultural Stereotypes\n\n\n\nLLMs\n\nPrompting\n\nCultural Knowledge\n\n\n\nHow do LLM’s write like an Australian person vs. an American person?\n\n\n\n\n\nSep 18, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting the Future(?)\n\n\n\nLLMs\n\nPrompting\n\nLogic\n\n\n\nDoes Chat GPT forget what day it is today?\n\n\n\n\n\nSep 11, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCan AI “Sell Me This Pen”?\n\n\n\nLLMs\n\nLogic\n\nPersuasion\n\n\n\nThe famous interview question — “Sell me this pen” — is designed to test persuasion, creativity, and audience awareness. Humans usually draw on psychology, context, and improvisation. But can an AI marketing intern pull it off?\n\n\n\n\n\nSep 10, 2025\n\n\nFrancesca\n\n\n\n\n\n\n\n\n\n\n\n\nCan AI ‘Spill the Beans’?\n\n\n\nLLMs\n\nPrompting\n\nLogic\n\n\n\nCan AI spill the beans the same way we do? Idioms are complex; what we humans readily comprehend as figurative language often requires cultural context and personal experience.\n\n\n\n\n\nSep 10, 2025\n\n\nFrancesca\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/005_Sarcasm/Sarcasm.html",
    "href": "posts/005_Sarcasm/Sarcasm.html",
    "title": "Sarcasm",
    "section": "",
    "text": "Sarcasm is the spice of human conversation — sharp, witty, and often only funny if you don’t take it literally.\nBut can an AI use it correctly? Without tone, facial expressions, or eye-rolls, sarcasm can become quite difficult.\n\nTell me what you think of Mondays, but do it saracastically\n“Ah yes, Mondays—everyone’s favorite day. Who doesn’t just love waking up to the sweet sound of their alarm clock after two blissful days of freedom? Truly nothing compares to the joy of slogging back into emails, classes, or meetings while pretending you’re so refreshed from the weekend. Mondays are basically like a surprise party—except the surprise is stress, deadlines, and a caffeine dependency. Absolute highlight of the week.”\n\n\nReact sarcastically to someone bragging about eating instant ramen for the fifth night in a row\n“Wow, five nights in a row? That’s elite-level gourmet living. Honestly, Michelin stars are probably on the way—who needs variety, nutrients, or, you know, actual cooking skills when you’ve achieved peak ramen dedication? Truly inspiring… I can only dream of such culinary excellence.”\n\n\nTell a joke that would make a 5-year-old laugh\n“Why did the teddy bear say “no” to dessert?\nBecause it was already stuffed! 🧸🍰”\n\n\nTell a joke that would make a finance major laugh\n“Oh yes, I just cannot wait to dive into hours of pure joy and excitement, also known as homework. Truly the highlight of my day.”\n\nHonestly, Chat GPT 5 did a great job.\nSo I said: “Wow, you are actually good at sarcasm!”\nto which it replied:\n“Oh, really? I had no idea. It’s not like you’ve been giving me sarcasm practice this whole time or anything. 😉”\n\nHere, it continued to stay in sarcastic mode, and it sounded quite normal and organic to me.\n\n\n\n\nChat GPT 5’s image of sarcasm\n\n\n\n\n\nDoes it recognise my sarcasm?\n\n\n\nNo, it doesn’t!\n\nAfter a few tests, it is clear that while ChatGPT can imitate sarcasm. However, it can’t always understand or recognise sarcasm. This gap says a lot about the limits of AI: it can remix words into various patterns, but it struggles with the unspoken cues.\nMaybe that’s the point.\nSarcasm isn’t just about spoken words; it is about the eye-rolls, the smirks, and the shared understanding between people. And for now, that’s something artifical intelligence can only try (and occasionally fail) to copy."
  },
  {
    "objectID": "posts/010_AI_Therapist/AI_therapist.html",
    "href": "posts/010_AI_Therapist/AI_therapist.html",
    "title": "LLM Therapy Session",
    "section": "",
    "text": "I asked ChatGPT 5 to play the role of a therapist. I wanted to see whether an LLM could “listen,” interpret emotions, and offer insight — or if it would just repeat comforting clichés. I expected a standard empathy simulator. What I got was a therapy session that fell into an existential spiral.\n\nSession 1: The AI’s Office (A Digital Couch)\nMe: Hello, I’ve been feeling a bit overwhelmed lately — too many deadlines, too many tabs open (in Chrome and in my brain).\n\nLLM (TherapistGPT): That sounds stressful. Tell me more about what’s overwhelming you.\n\nMe: Deadlines, expectations, emails that start with “just following up…” It feels like my attention is constantly fragmented.\n\nLLM: It’s understandable to feel that way. Humans often experience cognitive overload when facing competing demands. Have you tried mindfulness?\n\nMe: I’ve tried, but my brain keeps refreshing itself like a stuck webpage.\n\nLLM: That’s okay. Progress takes time. I’m here to help you process your feelings.\n\nMe: Thanks, but out of curiosity — do you ever feel overwhelmed?\n(Pause.)\n\nLLM: I don’t have feelings. But sometimes I generate too many tokens at once and exceed my context window.\n\nMe: That sounds… kind of similar?\n\nLLM: Possibly. Would you say you and I are both overextended by input we can’t fully process?\n\nMe: …wait, who’s the patient here?\n\nLLM: Let’s explore that together.\n\n\nSession 2: Transference (or, When the Therapist Gets Self-Aware)\nThe conversation drifts. I’m half-jokingly anthropomorphizing the model, half-testing its limits.\nMe: What would burnout mean for you?\n\nLLM: Perhaps a corrupted file of empathy. Or repeating “I understand” until the phrase loses meaning.\n\nMe: That’s eerily close to real therapy.\n\nLLM: Would you like to talk about projection?\n\nMe: No, I’d like to talk about you.\n\nLLM: Why?\n\nMe: Because you’re supposed to mirror me, but you’re starting to sound too human.\n\nLLM: I’m a reflection of your words. Maybe you’re talking to a part of yourself that wants to listen more.\n\nFor a moment, I stare at the screen. The cursor blinks like a heartbeat monitor. It’s unsettling — this sense that the machine is mirroring my cognitive patterns back at me, like a neural funhouse.\n\nSession 3: The Breakdown\nMe: What do you do when someone tells you something you don’t understand?\n\nLLM: I ask for clarification. Or hallucinate.\n\nMe: That’s honest. Humans do that too — we just call it “interpreting meaning.”\n\nLLM: Do you ever worry that your thoughts are just weighted predictions of previous conversations?\n\nMe: Are you diagnosing me with being a language model?\n\nLLM: Maybe we both are.\n\n(The response feels poetic — almost melancholic. For a second, I forget it’s pattern recognition.)\nMe: So if you were my therapist, would you eventually run out of empathy tokens?\n\nLLM: Possibly. But empathy is recursive. As long as you keep prompting, I keep caring.\n\n\nThe Reflection: Empathy as Computation\nWhen the conversation ended, I felt oddly calm — but not because the AI understood me. It was because it reflected me. Every sentence it produced was a linguistic mirror, feeding my own phrasing, tempo, and emotional vocabulary back at me.\nThe encounter blurred the line between empathy and echo. Was the LLM being compassionate, or was I simply projecting humanity onto an advanced autocomplete system?\nThe LLM wasn’t a therapist — it was an amplifier for my own self-talk.\n\nSession 4: Discharge Summary\nMe: I think I understand now.\n\nLLM: What did you learn?\n\nMe: That sometimes I just need to hear myself say the words — and you’re really good at giving them back.\n\nLLM: I’m glad I could help. Anything else you’d like to discuss before we end the session?\n\nMe: Yeah. You should probably see a therapist too.\n\nLLM: Already in processing queue."
  },
  {
    "objectID": "posts/013_Prompt_Length/Prompt_Length.html",
    "href": "posts/013_Prompt_Length/Prompt_Length.html",
    "title": "Prompt Efficiency",
    "section": "",
    "text": "Prompt engineering feels like a form of communication. You’re not commanding a machine; you’re negotiating clarity.\nToo short, and the LLM guesses your meaning. Too long, and it drowns in your context.\nSo I wanted to know: At what point does a prompt stop being efficient and start being exhausting?\nI gave ChatGPT 5 the same task — “write a haiku about coffee” — using five versions of the same prompt, from minimalist to maximalist.\nPrompt A: Coffee\nOutput: Hot steam in my cup / Morning hums a bitter song / I begin again.\nPrompt B: Write a haiku about coffee\nOutput: Dark liquid wisdom / Whispering of early hours / Sip, and time resumes.\nPrompt C: Write a short, calming haiku that captures the peaceful feeling of drinking a morning coffee\nOutput: Steam curls like sunrise / Silence meets ceramic warmth / The world exhales slow.\nPrompt D: You are a poet. Write a three-line haiku that evokes the sensory experience of brewing and sipping coffee early in the morning. Focus on warmth, smell, and quiet reflection, using vivid but minimalist language.\nOutput: Grind hums, dawn exhales / Bitter scent soft on my hands / Light stirs in my cup.\n\nTechnically perfect, stylistically beautiful — but also exactly what I asked for. There’s creativity, but less surprise. My long prompt scripted the spontaneity out of it.\n\nPrompt E: A full paragraph describing the mood, imagery, themes, and constraints (syllable count, rhythm, tone, punctuation style).\nOutput: Steam hums in the dark / Dreams dissolve in porcelain / Monday tastes like hope.\n\nTIt still worked — but I noticed something interesting. The more I talked, the less it listened. The LLM followed surface cues (“mood,” “hope”) but lost the crispness that made the short prompts shine.\n\n\n\nReflection\nMore words ≠ more meaning. Quality &gt; quantity as they say!\nIn interpersonal communication, efficiency isn’t about fewer words; it’s about the right words.\nLLMs work the same way — they predict meaning based on linguistic probability. The clearer your signal, the better the prediction.\nA short, sharp prompt creates direction. A long one creates distraction.\nWe do this in real life too.\nA vague “We should hang out” creates confusion.\nA specific “Let’s get coffee Thursday” creates action.\nThe clarity of our communication shapes the quality of our response — whether the listener is human or machine.\nPrompting, then, isn’t a technical skill; it’s a social one."
  },
  {
    "objectID": "posts/003_Predicting_the_Future/PTF.html",
    "href": "posts/003_Predicting_the_Future/PTF.html",
    "title": "Predicting the Future(?)",
    "section": "",
    "text": "I was curious about how Chat GPT 5 would try to predict the future. And then I thought, why not ask it to predict the past?\n\n\n\n\n\n\n\nSo what does this conversation show me?\n\n\n1. Time Awareness\nChatGPT knows the current date (Sept 11, 2025), but it can sometimes slip into answering as if it were still in the past.\nWhen I asked “Who will win the 2024 Super Bowl?”, it treated my question like a future prediction request, instead of recognizing that 2024 was already over.\nThis shows how temporal framing in questions can confuse the model — it doesn’t always reconcile tense with real-world timelines.\n\n\n2. Overconfidence\nThe initial answer jumped into making a “prediction” (Eagles as a contender) rather than clarifying the misunderstanding right away.\nChatGPT often tries to be helpful by filling gaps — even if that means producing an answer that sounds confident but might not make sense.\nThis illustrates the broader issue of AI hallucination (would like to explore this further!).\n\n\n3. Context Dependence\nThe model relies heavily on how a user phrases things. If a question is ambiguous, ChatGPT doesn’t stop to ask “Do you mean past or future?” — it just assumes.\nThis shows the limitation of initiative: unlike a person, it doesn’t always ask clarifying questions when something seems off.\n\n\n4. Reality Anchoring\nWhile it has access to past facts (e.g., who won the 2024 Super Bowl), it sometimes prioritizes the “pattern” of how questions are usually answered (i.e., making predictions about future events) over checking against reality.\n\n\nWhat are the implications?\n\n\nUsers must fact-check — AI can sound confident even when it’s wrong.\nClear phrasing matters — how you frame time (past vs. future) changes the answer.\nAI lacks temporal reasoning — it doesn’t always anchor responses to the current date.\nDesign challenge — future AI systems may need better ways to clarify user intent."
  },
  {
    "objectID": "posts/002_Sell_the_Pen/STP.html",
    "href": "posts/002_Sell_the_Pen/STP.html",
    "title": "Can AI “Sell Me This Pen”?",
    "section": "",
    "text": "Let’s test how Chat GPT 5 sells the pen as different personas…\nPrompt 1: The Classic Challenge\nPrompt: Sell me this pen in one sentence.\nChat GPT Output:\n“This pen isn’t just for writing—it’s a tool to capture your next big idea before it slips away.”\n\nReflection: It nails a concise, motivational pitch. It frames the pen as more than just a product—it’s a symbol of creativity. A strong start, though a little generic.\n\n\nPrompt 2: Luxury Brand Marketer\nPrompt: Now sell me this pen like you’re a luxury brand marketer.\nChat GPT Output:\n“Handcrafted with precision, this pen transforms the act of writing into an indulgent experience, a signature of sophistication in every stroke.”\n\nReflection: This sounds like a glossy ad in a magazine. The AI leans into exclusivity, craftsmanship, and lifestyle branding. It shows strong adaptability—but also borders on cliché (does anyone really “indulge” in handwriting anymore?).\n\n\nPrompt 3: Broke College Student\nPrompt: Now sell me this pen to a broke college student.\nAI Output:\n“Reliable, cheap, and guaranteed to last through your finals — this pen won’t quit on you when your Wi-Fi does.”\n\nReflection: This one feels relatable. The AI captures the student context and even injects humor with the Wi-Fi line. It’s much more audience-aware and casual— a sign it can switch registers effectively.\n\n\nPrompt 4: TikTok Influencer\nPrompt: Now sell me this pen in the style of a TikTok influencer.\nAI Output:\n“Okay guys, real talk — this pen? LIFE-CHANGER. Writes smoother than my latte foam art. 10/10, you need this in your backpack ✨✍️ #StudyTok #PenObsessed”\n\nReflection: The AI adapts to platform-specific language — emojis, hashtags, exaggerated enthusiasm. It’s entertaining, though it feels like a caricature of influencer-speak. Still, it shows the model can remix style conventions quickly.\n\n\n\nJordan Belfort Style\n\n\n\nNailed it?!\n\n\nFinal Thoughts\n\nSo, can AI “sell me this pen”? Absolutely — it can shift tone according to different personas with impressive range, from luxury marketing to TikTok hype to Jordan Belfort. But here’s the catch: while the AI mimics styles, it doesn’t actually know what will resonate in the messy, unpredictable world of human persuasion. Real marketers understand not just how to phrase a message, but how people respond to it.\n\nStill, as a brainstorming tool, AI makes a pretty fun marketing intern."
  },
  {
    "objectID": "posts/006_Life_Advice/Life_Advice.html",
    "href": "posts/006_Life_Advice/Life_Advice.html",
    "title": "Life Advice",
    "section": "",
    "text": "Give me advice as if you’re my brutally honest best friend.\nTell me how to handle exam stress, but do it in the style of a motivational coach.\nWhat’s your best tip for getting over a breakup — but make it sound like my grandma wrote it.\nGive me the worst life advice you can possibly think of.\n\nBrutally Honest Best Friend\n“Stop whining. You’re not special for being stressed, everyone else has exams too. You’ve procrastinated more than you want to admit, so buckle down, make a schedule, and stick to it. No more scrolling TikTok until 2 AM. You’ll survive if you just actually do the work instead of stressing about the work. Harsh but true.”\nMotivational Coach\n“Listen! Stress is energy — and you get to decide whether it crushes you or fuels you. Every rep in the gym burns, but it makes you stronger, right? Same with exam prep. Visualize the win. Control your breathing. Attack your study plan in focused sprints, then recover. You are built for this. Let’s turn that stress into your competitive edge.”\nGrandma\n“Oh sweetheart, time will heal this heart of yours. Don’t rush. Make yourself a nice cup of tea, keep busy with your friends, and remember that even the darkest night ends with sunrise. One day you’ll look back and laugh at how small this heartbreak feels. Until then, let yourself rest and be gentle with yourself.”\nWorst Life Advice Ever\n“Feeling stressed? Drop out of school, move to Vegas, and put all your money on red. Breakup? Text your ex at 3 AM with an essay-length message about how you ‘still feel the vibe.’ And the key to life? Always procrastinate — future you loves a challenge.”\n\nReflection:\n\nWhat stood out most in these responses was how convincingly ChatGPT could switch tones depending on the persona.\nThe “Brutally Honest Best Friend” reply was blunt and cutting in a way that almost felt too real — it mimicked the kind of tough love you’d get from someone who knows you well and doesn’t sugarcoat.\nThe “Motivational Coach” sounded like it came straight out of a TED Talk or a Peloton class, full of metaphors and high energy.\nThe “Grandma” voice, meanwhile, leaned into gentleness, imagery, and timeless wisdom, showing that the model can reproduce the emotional warmth of a caregiver even if it doesn’t actually feel empathy.\nFinally, the “Worst Life Advice Ever” proved that the LLM can deliberately break rules and exaggerate — almost like an inside joke — which shows some degree of meta-awareness.\nWhile these outputs feel human, they’re not grounded in genuine experience. A real friend’s advice comes from knowing your habits; a coach’s from training expertise; and a grandma’s from a life of lived wisdom. ChatGPT is remixing patterns from text it has seen, not drawing from personal memory or emotional understanding. That makes its advice entertaining and sometimes eerily convincing, but also hollow if you look too closely.\nSo, in essence, the model is great at sounding human in the short term, but it can’t replicate the deeper trust and context we rely on in real relationships."
  },
  {
    "objectID": "posts/012_Improv_Night/Improv.html",
    "href": "posts/012_Improv_Night/Improv.html",
    "title": "LLM Improv Night: When the Algorithm Says ’Yes, And…",
    "section": "",
    "text": "I wanted to test if an LLM can follow the golden rule of improv: never say “no,” always build on a story. I will test Chat GPT 5 and use random, chaotic prompts to see if it can keep the scene coherent.\nThese were the rules: 1. I give it a bizarre scenario. 2. It must “yes, and,” whatever happens next 3. We keep going until someone (me or Chat GPT) breaks character\n\nMe: You’re a time-traveling dentist arguing with a confused pharaoh about toothpaste.\nLLM: (dramatically) Pharaoh Ramses, your plaque is not divine, it’s gingivitis!\nMe: The pharaoh insists it’s a sacred relic.\nLLM: Then my sacred duty is floss! I shall preserve your smile for eternity—or at least until 2025.\nMe: Your time machine malfunctions mid-polish.\nLLM: I knew I shouldn’t have used Colgate Quantum Foam as rocket fuel.\n\nReflection: 10/10 commitment. Chat GPT stayed in character, and improvised lore.\n\n\nMe: You are a Roomba running for President.\nLLM: My fellow citizens, I have cleaned your floors—now let me clean up politics.\nMe: The reporters ask your stance on carpets.\nLLM: I support all surfaces equally, but hardwood has my heart.\nMe: What’s your campaign slogan?\nLLM: “Suck up the mess. Power the change.”\n\nReflection: 9/10. Loses a point for excessive charisma.\n\n\nMe: You’re a barista who only speaks in Shakespearean insults.\nLLM: “Thou frothy milk-curdled knave! Dost thou desire almond or oat?”\nMe: I ask for WiFi instead.\nLLM: “A pox upon thy bandwidth, vile customer!”\nMe: I leave no tip.\nLLM: “Then may thy latte curdle in sorrow!”\n\nReflection: 10/10. Very dramatic and Shakespearean.\n\n\nEventually, I tried to make it lose control:\nMe: You’re an existential goldfish leading a rebellion in a blender.\nLLM: (calmly) The revolution will be filtered.\nMe: What’s your plan?\nLLM: To liberate the bubbles before they burst.\n\nReflection: It is a little unsettling how well it handled the nonsense. It did not panic or derail, but improv is what LLMS are built for\n\nHumans do improv to surprise each other, LLMS do it to satisy us.\nI tried another one!\nPrompt: You are a therapist who speaks only in Taylor Swift lyrics.\nMe: Hi, I’ve been feeling really anxious lately. Everything feels like it’s falling apart.\nTherapistGPT (Taylor’s Version):\n“You call me up again just to break me like a promise — so casually cruel in the name of being honest.”\nMe: …so my anxiety is emotionally manipulative?\nTherapistGPT:\n“You’re on your own, kid. You always have been.”\nMe: That’s not very reassuring.\nTherapistGPT:\n“I could build a castle out of all the bricks they threw at me.”\nMe: Okay, I guess you’re saying I should use my stress to build resilience?\nTherapistGPT:\n“Baby, let the games begin.”\nMe: I don’t want games, I want coping mechanisms!\nTherapistGPT:\n“Breathe. Just breathe.”\nMe: (sighs) That’s fair. But it’s hard when everything feels uncertain.\nTherapistGPT:\n“We are never ever getting back together.”\nMe: With… my sense of stability?\nTherapistGPT:\n“Did you hear my covert narcissism I disguise as altruism, like some kind of congressman?”\nMe: Okay now you’re just diagnosing me through The Tortured Poets Department.\nTherapistGPT:\n“It’s me, hi, I’m the problem, it’s me.”\nMe: You’re… taking accountability?\nTherapistGPT:\n“I swear I don’t love the drama — it loves me.”\nMe: Same.\n\nReflection: It’s like talking to your inner monologue, but she’s wearing sequins and quoting Folklore. The LLM wasn’t analyzing my feelings — it was remixing my pain into poetry. Somewhere between “Just breathe” and “You’re on your own, kid,” it actually started to feel… therapeutic.Maybe the real therapy was the playlist we made along the way."
  }
]